{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd487927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nid004215\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669f89a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m4334/jerry/climsim3_dev\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb7561",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d960e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.transforms import blended_transform_factory\n",
    "import os, gc, sys, glob, string, argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import sys\n",
    "import pickle\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad218e99",
   "metadata": {},
   "source": [
    "# Set fontsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838cf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\":   14,\n",
    "    \"axes.labelsize\":   13,\n",
    "    \"figure.titlesize\": 17,\n",
    "    \"xtick.labelsize\":  11,\n",
    "    \"ytick.labelsize\":  11,\n",
    "    \"legend.fontsize\":  12,\n",
    "})\n",
    "\n",
    "def scale_default(param_name, scale_factor):\n",
    "    \"\"\"Get scaled version of default rcParam\"\"\"\n",
    "    return plt.rcParams[param_name] * scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871cc54",
   "metadata": {},
   "source": [
    "# Load utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaa54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = '/global/cfs/cdirs/m4334/jerry/climsim3_dev/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "\n",
    "input_mean_v2_rh_mc_file = 'input_mean_v2_rh_mc_pervar.nc'\n",
    "\n",
    "input_max_v2_rh_mc_file = 'input_max_v2_rh_mc_pervar.nc'\n",
    "input_min_v2_rh_mc_file = 'input_min_v2_rh_mc_pervar.nc'\n",
    "output_scale_v2_rh_mc_file = 'output_scale_std_lowerthred_v2_rh_mc.nc'\n",
    "\n",
    "input_mean_v6_file = 'input_mean_v6_pervar.nc'\n",
    "input_max_v6_file = 'input_max_v6_pervar.nc'\n",
    "input_min_v6_file = 'input_min_v6_pervar.nc'\n",
    "output_scale_v6_file = 'output_scale_std_lowerthred_v6.nc'\n",
    "\n",
    "lbd_qn_file = 'qn_exp_lambda_large.txt'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "grid_area = grid_info['area'].values\n",
    "area_weight = grid_area/np.sum(grid_area)\n",
    "level = grid_info.lev.values\n",
    "\n",
    "input_mean_v2_rh_mc = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_mean_v2_rh_mc_file)\n",
    "input_max_v2_rh_mc = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_max_v2_rh_mc_file)\n",
    "input_min_v2_rh_mc = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_min_v2_rh_mc_file)\n",
    "output_scale_v2_rh_mc = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/outputs/' + output_scale_v2_rh_mc_file)\n",
    "\n",
    "input_mean_v6 = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_mean_v6_file)\n",
    "input_max_v6 = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_max_v6_file)\n",
    "input_min_v6 = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + input_min_v6_file)\n",
    "output_scale_v6 = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/outputs/' + output_scale_v6_file)\n",
    "\n",
    "lbd_qn = np.loadtxt('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/' + lbd_qn_file, delimiter = ',')\n",
    "\n",
    "data_v2_rh_mc = data_utils(grid_info = grid_info, \n",
    "                           input_mean = input_mean_v2_rh_mc, \n",
    "                           input_max = input_max_v2_rh_mc, \n",
    "                           input_min = input_min_v2_rh_mc, \n",
    "                           output_scale = output_scale_v2_rh_mc,\n",
    "                           qinput_log = False,\n",
    "                           normalize = False)\n",
    "data_v2_rh_mc.set_to_v2_rh_mc_vars()\n",
    "\n",
    "data_v6 = data_utils(grid_info = grid_info,\n",
    "                     input_mean = input_mean_v6,\n",
    "                     input_max = input_max_v6,\n",
    "                     input_min = input_min_v6,\n",
    "                     output_scale = output_scale_v6,\n",
    "                     qinput_log = False,\n",
    "                     normalize = False)                     \n",
    "data_v6.set_to_v6_vars()\n",
    "\n",
    "actual_input_v2_rh_mc = np.load('/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/actual_input.npy')\n",
    "actual_target_v2_rh_mc = np.load('/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/actual_target.npy')\n",
    "\n",
    "actual_input_v6 = np.load('/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v6/test_set/actual_input.npy')\n",
    "actual_target_v6 = np.load('/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v6/test_set/actual_target.npy')\n",
    "\n",
    "assert np.array_equal(actual_target_v2_rh_mc, actual_target_v6)\n",
    "actual_target = np.load('/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/original_test_vars/actual_target_v2.npy')\n",
    "del actual_target_v2_rh_mc\n",
    "del actual_target_v6\n",
    "\n",
    "assert np.array_equal(actual_input_v2_rh_mc[:,:,data_v2_rh_mc.ps_index], \n",
    "                      actual_input_v6[:,:,data_v6.ps_index])\n",
    "\n",
    "surface_pressure = actual_input_v2_rh_mc[:, :, data_v2_rh_mc.ps_index]\n",
    "hyam_component = (data_v2_rh_mc.hyam * data_v2_rh_mc.p0)[None,None,:]\n",
    "hybm_component = data_v2_rh_mc.hybm[None,None,:] * surface_pressure[:,:,None]\n",
    "pressures = hyam_component + hybm_component\n",
    "pressures_binned = data_v2_rh_mc.zonal_bin_weight_3d(pressures)\n",
    "lat_bin_mids = data_v2_rh_mc.lat_bin_mids\n",
    "lats = data_v2_rh_mc.lats\n",
    "lons = data_v2_rh_mc.lons\n",
    "\n",
    "idx_p400_t10 = np.load('/pscratch/sd/z/zeyuanhu/hu_etal2024_data/microphysics_hourly/first_true_indices_p400_t10.npy')\n",
    "for i in range(idx_p400_t10.shape[0]):\n",
    "    for j in range(idx_p400_t10.shape[1]):\n",
    "        idx_p400_t10[i,j] = level[int(idx_p400_t10[i,j])]\n",
    "\n",
    "idx_p400_t10 = idx_p400_t10.mean(axis=0)\n",
    "idx_p400_t10 = idx_p400_t10[np.newaxis,:]\n",
    "\n",
    "idx_tropopause_zm = data_v2_rh_mc.zonal_bin_weight_2d(idx_p400_t10).flatten()\n",
    "\n",
    "area_weight_dict = {\n",
    "    'global': area_weight,\n",
    "    'nh': np.where(lats > 30, area_weight, 0),\n",
    "    'sh': np.where(lats < -30, area_weight, 0),\n",
    "    'tropics': np.where((lats > -30) & (lats < 30), area_weight, 0)\n",
    "}\n",
    "\n",
    "lat_idx_dict = {\n",
    "    '30S_30N': ((data_v2_rh_mc.lats < 30) & (data_v2_rh_mc.lats > -30))[None,:,None],\n",
    "    '30N_60N': ((data_v2_rh_mc.lats < 60) & (data_v2_rh_mc.lats > 30))[None,:,None],\n",
    "    '30S_60S': ((data_v2_rh_mc.lats < -30) & (data_v2_rh_mc.lats > -60))[None,:,None],\n",
    "    '60N_90N': (data_v2_rh_mc.lats > 60)[None,:,None],\n",
    "    '60S_90S': (data_v2_rh_mc.lats < -60)[None,:,None]\n",
    "}\n",
    "\n",
    "pressure_idx_dict = {\n",
    "    'below_400hPa': pressures >= 400,\n",
    "    'above_400hPa': pressures < 400\n",
    "}\n",
    "\n",
    "config_names = {\n",
    "    'standard': 'Standard',\n",
    "    'conf_loss': 'Confidence Loss',\n",
    "    'diff_loss': 'Difference Loss',\n",
    "    'multirep': 'Multirepresentation',\n",
    "    'v6': 'Expanded Variable List'\n",
    "}\n",
    "\n",
    "model_names = {\n",
    "    'unet': 'U-Net',\n",
    "    'squeezeformer': 'Squeezeformer',\n",
    "    'pure_resLSTM': 'Pure ResLSTM',\n",
    "    'pao_model': 'Pao Model',\n",
    "    'convnext': 'ConvNeXt',\n",
    "    'encdec_lstm': 'Encoder-Decoder LSTM'\n",
    "}\n",
    "\n",
    "color_dict = {\n",
    "    'unet': 'green',\n",
    "    'squeezeformer': 'purple',\n",
    "    'pure_resLSTM': 'blue',\n",
    "    'pao_model': 'red',\n",
    "    'convnext': 'gold',\n",
    "    'encdec_lstm': 'orange',\n",
    "}\n",
    "\n",
    "color_dict_config = {\n",
    "    'standard': 'blue',\n",
    "    'conf_loss': 'cyan',\n",
    "    'diff_loss': 'red',\n",
    "    'multirep': 'orange',\n",
    "    'v6': 'green'\n",
    "}\n",
    "\n",
    "offline_var_settings = {\n",
    "    'DTPHYS': {'var_title': 'dT/dt', 'scaling': 1., 'unit': 'K/s', 'vmax': 5e-7, 'vmin': -5e-7, 'var_index':0},\n",
    "    'DQ1PHYS': {'var_title': 'dQv/dt', 'scaling': 1e3, 'unit': 'g/kg/s', 'vmax': 1e-6, 'vmin': -1e-6, 'var_index':60},\n",
    "    'DQ2PHYS': {'var_title': 'dQl/dt', 'scaling': 1e6, 'unit': 'mg/kg/s', 'vmax': 1e-3, 'vmin': -1e-3, 'var_index':120},\n",
    "    'DQ3PHYS': {'var_title': 'dQi/dt', 'scaling': 1e6, 'unit': 'mg/kg/s', 'vmax': 1e-3, 'vmin': -1e-3, 'var_index':180},\n",
    "    'DUPHYS': {'var_title': 'dU/dt', 'scaling': 1., 'unit': 'm/s/s', 'vmax': 5e-7, 'vmin': -5e-7, 'var_index':240},\n",
    "    'DVPHYS': {'var_title': 'dV/dt', 'scaling': 1., 'unit': 'm/s/s', 'vmax': 5e-7, 'vmin': -5e-7, 'var_index':300}\n",
    "}\n",
    "\n",
    "online_var_settings = {\n",
    "    'T': {'var_title': 'Temperature', 'scaling': 1.0, 'unit': 'K', 'vmax': 5, 'vmin': -5},\n",
    "    'Q': {'var_title': 'Specific Humidity', 'scaling': 1000.0, 'unit': 'g/kg', 'vmax': 1, 'vmin': -1},\n",
    "    'U': {'var_title': 'Zonal Wind', 'scaling': 1.0, 'unit': 'm/s', 'vmax': 4, 'vmin': -4},\n",
    "    'V': {'var_title': 'Meridional Wind', 'scaling': 1.0, 'unit': 'm/s', 'vmax': 4, 'vmin': -4},\n",
    "    'CLDLIQ': {'var_title': 'Liquid Cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'vmax': 40, 'vmin': -40},\n",
    "    'CLDICE': {'var_title': 'Ice Cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'vmax': 5, 'vmin': -5},\n",
    "    'TOTCLD': {'var_title': 'Total Cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'vmax': 40, 'vmin': -40},\n",
    "    'DTPHYS': {'var_title': 'Heating Tendency', 'scaling': 1., 'unit': 'K/s', 'vmax': 1.5e-5, 'vmin': -1.5e-5},\n",
    "    'DQ1PHYS': {'var_title': 'Moistening Tendency', 'scaling': 1e3, 'unit': 'g/kg/s', 'vmax': 1.2e-5, 'vmin': -1.2e-5},\n",
    "    'DQ2PHYS': {'var_title': 'Liquid Tendency', 'scaling': 1e6, 'unit': 'mg/kg/s', 'vmax': 0.0015, 'vmin': -0.0015},\n",
    "    'DQ3PHYS': {'var_title': 'Ice Tendency', 'scaling': 1e6, 'unit': 'mg/kg/s', 'vmax': 0.0015, 'vmin': -0.0015},\n",
    "    'DQnPHYS': {'var_title': 'Liquid + Ice Tendency', 'scaling': 1e6, 'unit': 'mg/kg/s', 'vmax': .0015, 'vmin': -.0015},\n",
    "    'DUPHYS': {'var_title': 'Zonal Wind Tendency', 'scaling': 1., 'unit': 'm/sÂ²', 'vmax': 2.2e-6, 'vmin': -2.2e-6}\n",
    "}\n",
    "\n",
    "online_paths = {\n",
    "    'standard': '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/standard/five_year_runs/',\n",
    "    'conf_loss': '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/conf/five_year_runs/',\n",
    "    'diff_loss': '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/diff/five_year_runs/',\n",
    "    'multirep': '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/multirep/five_year_runs/',\n",
    "    'v6': '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/'\n",
    "}\n",
    "\n",
    "seeds = ['seed_7', 'seed_43', 'seed_1024']\n",
    "seed_numbers = [7, 43, 1024]\n",
    "\n",
    "climsim3_figures_save_path_offline = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/offline'\n",
    "climsim3_figures_save_path_online = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online'\n",
    "\n",
    "sypd_standard = [\n",
    "    17.97901035,\n",
    "    17.97901035,\n",
    "    18.05589744,\n",
    "    18.90966668,\n",
    "    19.03178614,\n",
    "    18.65975565,\n",
    "    17.80983045,\n",
    "    18.08638973,\n",
    "    17.93128756,\n",
    "    21.78946853,\n",
    "    21.9847479,\n",
    "    22.20038272,\n",
    "    25.36398855,\n",
    "    25.19805864,\n",
    "    25.19227595,\n",
    "    23.54557187,\n",
    "    23.82794497,\n",
    "    23.447502\n",
    "]\n",
    "\n",
    "sypd_conf_loss = [\n",
    "    17.50453936,\n",
    "    17.29971515,\n",
    "    17.44196107,\n",
    "    18.20260208,\n",
    "    18.19053688,\n",
    "    18.04921777,\n",
    "    16.88591639,\n",
    "    17.06639861,\n",
    "    17.10229289,\n",
    "    20.55525559,\n",
    "    20.63385549,\n",
    "    20.42521955,\n",
    "    23.65465549,\n",
    "    23.3813296,\n",
    "    23.25625602,\n",
    "    22.31773163,\n",
    "    22.25777422,\n",
    "    21.88176458\n",
    "]\n",
    "\n",
    "sypd_diff_loss =[\n",
    "    16.97469344,\n",
    "    16.92105472,\n",
    "    17.16379509,\n",
    "    18.99145235,\n",
    "    19.24026684,\n",
    "    18.69869139,\n",
    "    17.75509967,\n",
    "    16.87358759,\n",
    "    16.66731672,\n",
    "    20.77602886,\n",
    "    19.70212994,\n",
    "    19.22005471,\n",
    "    21.00260056,\n",
    "    25.47730606,\n",
    "    25.47730606,\n",
    "    24.0893883,\n",
    "    23.99985625,\n",
    "    23.78792837\n",
    "]\n",
    "\n",
    "sypd_multirep = [\n",
    "    18.33255552,\n",
    "    18.35861568,\n",
    "    18.3448099,\n",
    "    19.00295884,\n",
    "    16.93296157,\n",
    "    18.19204416,\n",
    "    17.23723679,\n",
    "    17.27385405,\n",
    "    17.27657263,\n",
    "    18.10913991,\n",
    "    19.86397164,\n",
    "    21.35810934,\n",
    "    23.51060144,\n",
    "    23.60014809,\n",
    "    24.52549157,\n",
    "    23.19360711,\n",
    "    23.43123639,\n",
    "    23.23164752\n",
    "]\n",
    "\n",
    "sypd_v6 = [\n",
    "    18.21284028,\n",
    "    18.31251632,\n",
    "    18.41758393,\n",
    "    6.894533848,\n",
    "    8.95130655,\n",
    "    7.890410959,\n",
    "    17.84446786,\n",
    "    17.79133537,\n",
    "    17.73373234,\n",
    "    21.39020294,\n",
    "    17.90628317,\n",
    "    21.40600448,\n",
    "    24.47883654,\n",
    "    24.13656852,\n",
    "    24.3122588,\n",
    "    23.34100196,\n",
    "    23.5582043,\n",
    "    23.5169076\n",
    "]\n",
    "\n",
    "pc_standard = [\n",
    "    12975373,\n",
    "    12975373,\n",
    "    12975373,\n",
    "    44785225,\n",
    "    44785225,\n",
    "    44785225,\n",
    "    15395341,\n",
    "    15395341,\n",
    "    15395341,\n",
    "    18876133,\n",
    "    18876133,\n",
    "    18876133,\n",
    "    26805429,\n",
    "    26805429,\n",
    "    26805429,\n",
    "    18582976,\n",
    "    18582976,\n",
    "    18582976\n",
    "]\n",
    "\n",
    "pc_conf_loss = [\n",
    "    12980634,\n",
    "    12980634,\n",
    "    12980634,\n",
    "    44811862,\n",
    "    44811862,\n",
    "    44811862,\n",
    "    15402010,\n",
    "    15402010,\n",
    "    15402010,\n",
    "    25407346,\n",
    "    25407346,\n",
    "    25407346,\n",
    "    26839242,\n",
    "    26839242,\n",
    "    26839242,\n",
    "    20723124,\n",
    "    20723124,\n",
    "    20723124\n",
    "]\n",
    "\n",
    "pc_diff_loss = [\n",
    "    12975373,\n",
    "    12975373,\n",
    "    12975373,\n",
    "    44785225,\n",
    "    44785225,\n",
    "    44785225,\n",
    "    15395341,\n",
    "    15395341,\n",
    "    15395341,\n",
    "    18876133,\n",
    "    18876133,\n",
    "    18876133,\n",
    "    26805429,\n",
    "    26805429,\n",
    "    26805429,\n",
    "    18582976,\n",
    "    18582976,\n",
    "    18582976\n",
    "]\n",
    "\n",
    "pc_multirep = [\n",
    "    12981517,\n",
    "    12981517,\n",
    "    12981517,\n",
    "    44791369,\n",
    "    44791369,\n",
    "    44791369,\n",
    "    15428109,\n",
    "    15428109,\n",
    "    15428109,\n",
    "    18880613,\n",
    "    18880613,\n",
    "    18880613,\n",
    "    26811573,\n",
    "    26811573,\n",
    "    26811573,\n",
    "    21004960,\n",
    "    21004960,\n",
    "    21004960\n",
    "]\n",
    "\n",
    "pc_v6 = [\n",
    "    12981517,\n",
    "    12981517,\n",
    "    12981517,\n",
    "    44791369,\n",
    "    44791369,\n",
    "    44791369,\n",
    "    15428109,\n",
    "    15428109,\n",
    "    15428109,\n",
    "    18880373,\n",
    "    18880373,\n",
    "    18880373,\n",
    "    26811573,\n",
    "    26811573,\n",
    "    26811573,\n",
    "    21004960,\n",
    "    21004960,\n",
    "    21004960\n",
    "]\n",
    "\n",
    "standard_sypd_dict = {}\n",
    "conf_loss_sypd_dict = {}\n",
    "diff_loss_sypd_dict = {}\n",
    "multirep_sypd_dict = {}\n",
    "v6_sypd_dict = {}\n",
    "\n",
    "standard_pc_dict = {}\n",
    "conf_loss_pc_dict = {}\n",
    "diff_loss_pc_dict = {}\n",
    "multirep_pc_dict = {}\n",
    "v6_pc_dict = {}\n",
    "\n",
    "i = 0\n",
    "for model_name in model_names.keys():\n",
    "    for seed_number in seed_numbers:\n",
    "        standard_sypd_dict[f\"{model_name}_{seed_number}\"] = sypd_standard[i]\n",
    "        conf_loss_sypd_dict[f\"{model_name}_{seed_number}\"] = sypd_conf_loss[i]\n",
    "        diff_loss_sypd_dict[f\"{model_name}_{seed_number}\"] = sypd_diff_loss[i]\n",
    "        multirep_sypd_dict[f\"{model_name}_{seed_number}\"] = sypd_multirep[i]\n",
    "        v6_sypd_dict[f\"{model_name}_{seed_number}\"] = sypd_v6[i]\n",
    "        standard_pc_dict[f\"{model_name}_{seed_number}\"] = pc_standard[i]\n",
    "        conf_loss_pc_dict[f\"{model_name}_{seed_number}\"] = pc_conf_loss[i]\n",
    "        diff_loss_pc_dict[f\"{model_name}_{seed_number}\"] = pc_diff_loss[i]\n",
    "        multirep_pc_dict[f\"{model_name}_{seed_number}\"] = pc_multirep[i]\n",
    "        v6_pc_dict[f\"{model_name}_{seed_number}\"] = pc_v6[i]\n",
    "        i += 1\n",
    "\n",
    "sypd_dict = {\n",
    "    'standard': standard_sypd_dict,\n",
    "    'conf_loss': conf_loss_sypd_dict,\n",
    "    'diff_loss': diff_loss_sypd_dict,\n",
    "    'multirep': multirep_sypd_dict,\n",
    "    'v6': v6_sypd_dict\n",
    "}\n",
    "pc_dict = {\n",
    "    'standard': standard_pc_dict,\n",
    "    'conf_loss': conf_loss_pc_dict,\n",
    "    'diff_loss': diff_loss_pc_dict,\n",
    "    'multirep': multirep_pc_dict,\n",
    "    'v6': v6_pc_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b40463",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5455c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(data_path = \"\"):\n",
    "    return os.popen(\" \".join([\"ls\", data_path])).read().splitlines()\n",
    "\n",
    "def offline_area_time_mean_3d(arr):\n",
    "    arr_zonal_mean = data_v2_rh_mc.zonal_bin_weight_3d(arr)\n",
    "    arr_zonal_time_mean = arr_zonal_mean.mean(axis = 0)\n",
    "    arr_zonal_time_mean = xr.DataArray(arr_zonal_time_mean.T, dims = ['hybrid pressure (hPa)', 'latitude'], coords = {'hybrid pressure (hPa)':level, 'latitude': lat_bin_mids})\n",
    "    return arr_zonal_time_mean\n",
    "\n",
    "def online_area_time_mean_3d(ds, var):\n",
    "    arr = ds[var].values[1:,:,:]\n",
    "    arr_reshaped = np.transpose(arr, (0,2,1))\n",
    "    arr_zonal_mean = data_v2_rh_mc.zonal_bin_weight_3d(arr_reshaped)\n",
    "    arr_zonal_time_mean = arr_zonal_mean.mean(axis = 0)\n",
    "    arr_zonal_time_mean = xr.DataArray(arr_zonal_time_mean.T, dims = ['hybrid pressure (hPa)', 'latitude'], coords = {'hybrid pressure (hPa)':level, 'latitude': lat_bin_mids})\n",
    "    return arr_zonal_time_mean\n",
    "\n",
    "def area_mean(ds, var):\n",
    "    arr = ds[var].values\n",
    "    arr_reshaped = np.transpose(arr, (0,2,1))\n",
    "    arr_zonal_mean = data_v2_rh_mc.zonal_bin_weight_3d(arr_reshaped)\n",
    "    return arr_zonal_mean\n",
    "\n",
    "def zonal_diff(ds_sp, ds_nn, var):\n",
    "    diff_zonal_mean = (area_mean(ds_nn, var) - area_mean(ds_sp, var)).mean(axis = 0)\n",
    "    diff_zonal = xr.DataArray(diff_zonal_mean.T, dims = ['level', 'lat'], coords = {'level':level, 'lat': lat_bin_mids})\n",
    "    return diff_zonal\n",
    "\n",
    "def get_dp(ds):\n",
    "    ps = ds['PS']\n",
    "    p_interface = (ds['hyai'] * ds['P0'] + ds['hybi'] * ds['PS']).values\n",
    "    if p_interface.shape[0] == 61:\n",
    "        p_interface = np.swapaxes(p_interface, 0, 1)\n",
    "    dp = p_interface[:,1:61,:] - p_interface[:,0:60,:]\n",
    "    return dp\n",
    "\n",
    "def get_tcp_mean(ds, area_weight):\n",
    "    cld = ds['TOTCLD'].values\n",
    "    dp = get_dp(ds)\n",
    "    tcp = np.sum(cld*dp, axis = 1)/9.81\n",
    "    tcp_mean = np.average(tcp, weights = area_weight, axis = 1)\n",
    "    return tcp_mean\n",
    "\n",
    "def get_tcp_std(ds, area_weight):\n",
    "    cld = ds['TOTCLD'].values\n",
    "    dp = get_dp(ds)\n",
    "    tcp = np.sum(cld*dp, axis = 1)/9.81\n",
    "    tcp_mean = np.average(tcp, weights = area_weight, axis = 1)\n",
    "    squared_diff = (tcp - tcp_mean[:, None])**2\n",
    "    tcp_std = np.sqrt(np.average(squared_diff, weights = area_weight, axis = 1))\n",
    "    return tcp_std\n",
    "\n",
    "def read_mmf_online_data(num_years):\n",
    "    assert num_years <= 5 and num_years >= 1\n",
    "    years_regexp = '34567'[:num_years]\n",
    "    ds_mmf_1 = xr.open_mfdataset(f'/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_ref/control_fullysp_jan_wmlio_r3.eam.h0.000[{years_regexp}]*.nc')\n",
    "    ds_mmf_2 = xr.open_mfdataset(f'/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_b/control_fullysp_jan_wmlio_r3_b.eam.h0.000[{years_regexp}]*.nc')\n",
    "    ds_mmf_1['DQnPHYS'] = ds_mmf_1['DQ2PHYS'] + ds_mmf_1['DQ3PHYS']\n",
    "    ds_mmf_2['DQnPHYS'] = ds_mmf_2['DQ2PHYS'] + ds_mmf_2['DQ3PHYS']\n",
    "    ds_mmf_1['TOTCLD'] = ds_mmf_1['CLDICE'] + ds_mmf_1['CLDLIQ']\n",
    "    ds_mmf_2['TOTCLD'] = ds_mmf_2['CLDICE'] + ds_mmf_2['CLDLIQ']\n",
    "    ds_mmf_1['PRECT'] = ds_mmf_1['PRECC'] + ds_mmf_1['PRECL']\n",
    "    ds_mmf_2['PRECT'] = ds_mmf_2['PRECC'] + ds_mmf_2['PRECL']\n",
    "    return ds_mmf_1, ds_mmf_2\n",
    "\n",
    "def read_nn_online_data(config_name, model_name, seed, num_years):\n",
    "    assert num_years <= 5 and num_years >= 1\n",
    "    years_regexp = '34567'[:num_years]\n",
    "    if config_name == 'standard':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_seed_{seed}', 'run', f'{model_name}_seed_{seed}.eam.h0.000[{years_regexp}]*.nc')\n",
    "    elif config_name == 'conf_loss':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_conf_seed_{seed}', 'run', f'{model_name}_conf_seed_{seed}.eam.h0.000[{years_regexp}]*.nc')\n",
    "    elif config_name == 'diff_loss':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_diff_seed_{seed}', 'run', f'{model_name}_diff_seed_{seed}.eam.h0.000[{years_regexp}]*.nc')\n",
    "    elif config_name == 'multirep':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_multirep_seed_{seed}', 'run', f'{model_name}_multirep_seed_{seed}.eam.h0.000[{years_regexp}]*.nc')\n",
    "    elif config_name == 'v6':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_v6_seed_{seed}', 'run', f'{model_name}_v6_seed_{seed}.eam.h0.000[{years_regexp}]*.nc')\n",
    "    if len(ls(extract_path)) == 0:\n",
    "        return None\n",
    "    ds_nn = xr.open_mfdataset(extract_path)\n",
    "    if len(ds_nn['time']) < 12 * num_years:\n",
    "        return None\n",
    "    ds_nn['DQnPHYS'] = ds_nn['DQ2PHYS'] + ds_nn['DQ3PHYS']\n",
    "    ds_nn['TOTCLD'] = ds_nn['CLDICE'] + ds_nn['CLDLIQ']\n",
    "    ds_nn['PRECT'] = ds_nn['PRECC'] + ds_nn['PRECL']\n",
    "    return ds_nn\n",
    "\n",
    "def read_nn_online_precip_data(config_name, model_name, seed, num_years):\n",
    "    assert num_years <= 5 and num_years >= 1\n",
    "    years_regexp = '34567'[:num_years]\n",
    "    if config_name == 'standard':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_seed_{seed}', 'run', 'precip_dir', 'combined_precip.nc')\n",
    "    elif config_name == 'conf_loss':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_conf_seed_{seed}', 'run', 'precip_dir', 'combined_precip.nc')\n",
    "    elif config_name == 'diff_loss':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_diff_seed_{seed}', 'run', 'precip_dir', 'combined_precip.nc')\n",
    "    elif config_name == 'multirep':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_multirep_seed_{seed}', 'run', 'precip_dir', 'combined_precip.nc')\n",
    "    elif config_name == 'v6':\n",
    "        extract_path = os.path.join(online_paths[config_name], f'{model_name}_v6_seed_{seed}', 'run', 'precip_dir', 'combined_precip.nc')\n",
    "    if len(ls(extract_path)) == 0:\n",
    "        return None\n",
    "    ds_nn = xr.open_dataset(extract_path)\n",
    "    if len(ds_nn['time']) < 365 * 24 * num_years:\n",
    "        return None\n",
    "    return ds_nn['PRECT']\n",
    "\n",
    "def get_pressure_area_weights(ds, surface_type = None):\n",
    "    ds_dp = get_dp(ds)\n",
    "    ds_total_weight = ds_dp * area_weight[None, None, :]\n",
    "    ds_total_weight = ds_total_weight.mean(axis = 0)\n",
    "    ds_total_weight = ds_total_weight/ds_total_weight.sum()\n",
    "    if surface_type is None:\n",
    "        return ds_total_weight\n",
    "    elif surface_type == 'land':\n",
    "        land_area = ds['LANDFRAC'].values * grid_area[None, :]\n",
    "        land_area_sums = np.array([[np.sum(land_area[t,:][data_v2_rh_mc.lat_bin_dict[lat_bin]]) for lat_bin in data_v2_rh_mc.lat_bin_dict.keys()] for t in range(land_area.shape[0])])\n",
    "        land_area_divs = np.stack([np.divide(1, land_area_sums[:, bin_index], where=~(land_area_sums[:, bin_index] == 0), out=np.zeros_like(land_area_sums[:, bin_index])) for bin_index in data_v2_rh_mc.lat_bin_indices], axis=1)\n",
    "        land_area_weighting = land_area * land_area_divs\n",
    "        return land_area_weighting\n",
    "    elif surface_type == 'ocean':\n",
    "        ocean_area = ds['OCNFRAC'].values * grid_area[None, :]\n",
    "        ocean_area_sums = np.array([[np.sum(ocean_area[t,:][data_v2_rh_mc.lat_bin_dict[lat_bin]]) for lat_bin in data_v2_rh_mc.lat_bin_dict.keys()] for t in range(ocean_area.shape[0])])\n",
    "        ocean_area_divs = np.stack([np.divide(1, ocean_area_sums[:, bin_index], where=~(ocean_area_sums[:, bin_index] == 0), out=np.zeros_like(ocean_area_sums[:, bin_index])) for bin_index in data_v2_rh_mc.lat_bin_indices], axis=1)\n",
    "        ocean_area_weighting = ocean_area * ocean_area_divs\n",
    "        return ocean_area_weighting\n",
    "    elif surface_type == 'ice':\n",
    "        ice_area = ds['ICEFRAC'].values * grid_area[None, :]\n",
    "        ice_area_sums = np.array([[np.sum(ice_area[t,:][data_v2_rh_mc.lat_bin_dict[lat_bin]]) for lat_bin in data_v2_rh_mc.lat_bin_dict.keys()] for t in range(ice_area.shape[0])])\n",
    "        ice_area_divs = np.stack([np.divide(1, ice_area_sums[:, bin_index], where=~(ice_area_sums[:, bin_index] == 0), out=np.zeros_like(ice_area_sums[:, bin_index])) for bin_index in data_v2_rh_mc.lat_bin_indices], axis=1)\n",
    "        ice_area_weighting = ice_area * ice_area_divs\n",
    "        return ice_area_weighting\n",
    "    else:\n",
    "        raise ValueError(\"Invalid surface type. Choose from 'land', 'ocean', or 'ice'.\")\n",
    "\n",
    "def get_offline_precip_area_weights(nn_preds, precc_index = 363):\n",
    "    nn_precc = nn_preds[:,:,precc_index] * 86400 * 1000\n",
    "    no_precip_mask = nn_precc[:,:] == 0\n",
    "    nn_active_precip = nn_precc[:,:][~no_precip_mask]\n",
    "    nn_active_precip_quantiles = np.quantile(nn_active_precip, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    active_precip_1_10_percentile_mask = (nn_precc[:,:] > 0) & (nn_precc[:,:] <= nn_active_precip_quantiles[0])\n",
    "    active_precip_10_20_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[0]) & (nn_precc[:,:] <= nn_active_precip_quantiles[1])\n",
    "    active_precip_20_30_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[1]) & (nn_precc[:,:] <= nn_active_precip_quantiles[2])\n",
    "    active_precip_30_40_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[2]) & (nn_precc[:,:] <= nn_active_precip_quantiles[3])\n",
    "    active_precip_40_50_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[3]) & (nn_precc[:,:] <= nn_active_precip_quantiles[4])\n",
    "    active_precip_50_60_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[4]) & (nn_precc[:,:] <= nn_active_precip_quantiles[5])\n",
    "    active_precip_60_70_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[5]) & (nn_precc[:,:] <= nn_active_precip_quantiles[6])\n",
    "    active_precip_70_80_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[6]) & (nn_precc[:,:] <= nn_active_precip_quantiles[7])\n",
    "    active_precip_80_90_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[7]) & (nn_precc[:,:] <= nn_active_precip_quantiles[8])\n",
    "    active_precip_90_100_percentile_mask = (nn_precc[:,:] > nn_active_precip_quantiles[8]) & (nn_precc[:,:] <= nn_active_precip_quantiles[9])\n",
    "    precip_area_dict = {\n",
    "        'no_precip': no_precip_mask * area_weight[None,:],\n",
    "        'active_precip_1_10_percentile': active_precip_1_10_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_10_20_percentile': active_precip_10_20_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_20_30_percentile': active_precip_20_30_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_30_40_percentile': active_precip_30_40_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_40_50_percentile': active_precip_40_50_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_50_60_percentile': active_precip_50_60_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_60_70_percentile': active_precip_60_70_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_70_80_percentile': active_precip_70_80_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_80_90_percentile': active_precip_80_90_percentile_mask * area_weight[None,:],\n",
    "        'active_precip_90_100_percentile': active_precip_90_100_percentile_mask * area_weight[None,:]\n",
    "    }\n",
    "    precip_area_divs = {key: np.divide(np.ones_like(precip_area_dict[key]), np.sum(precip_area_dict[key]), out = np.zeros_like(precip_area_dict[key]), where = (precip_area_dict[key] != 0)) for key in precip_area_dict.keys()}\n",
    "    for key in precip_area_dict.keys():\n",
    "        precip_area_dict[key] = (precip_area_dict[key] * precip_area_divs[key])[:,:,None]\n",
    "    return precip_area_dict\n",
    "\n",
    "precip_percentile_labels = {\n",
    "    'no_precip': 'No Precipitation',\n",
    "    'active_precip_1_10_percentile': '1-10th percentile',\n",
    "    'active_precip_10_20_percentile': '10-20th percentile',\n",
    "    'active_precip_20_30_percentile': '20-30th percentile',\n",
    "    'active_precip_30_40_percentile': '30-40th percentile',\n",
    "    'active_precip_40_50_percentile': '40-50th percentile',\n",
    "    'active_precip_50_60_percentile': '50-60th percentile',\n",
    "    'active_precip_60_70_percentile': '60-70th percentile',\n",
    "    'active_precip_70_80_percentile': '70-80th percentile',\n",
    "    'active_precip_80_90_percentile': '80-90th percentile',\n",
    "    'active_precip_90_100_percentile': '90-100th percentile'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca279764",
   "metadata": {},
   "source": [
    "# Load offline preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f4e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading standard preds\n",
      "loading conf loss preds\n",
      "loading conf loss conf\n",
      "loading diff loss preds\n",
      "loading multirep preds\n",
      "loading v6 preds\n",
      "loading Kaggle preds\n",
      "(4380, 384, 368) (4380, 384, 368)\n"
     ]
    }
   ],
   "source": [
    "standard_save_path = '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/test_preds/standard/'\n",
    "conf_loss_save_path = '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/test_preds/conf_loss/'\n",
    "diff_loss_save_path = '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/test_preds/diff_loss/'\n",
    "multirep_save_path = '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/test_set/test_preds/multirep/'\n",
    "v6_save_path = '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v6/test_set/test_preds/'\n",
    "\n",
    "def load_seed_data(save_path, npz_file, seed_key):\n",
    "    with np.load(os.path.join(save_path, npz_file)) as data:\n",
    "        return data[seed_key]\n",
    "\n",
    "print('loading standard preds')\n",
    "standard_preds = {\n",
    "    'unet': lambda seed_key: load_seed_data(standard_save_path, 'standard_unet_preds.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(standard_save_path, 'standard_squeezeformer_preds.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(standard_save_path, 'standard_pure_resLSTM_preds.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(standard_save_path, 'standard_pao_model_preds.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(standard_save_path, 'standard_convnext_preds.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(standard_save_path, 'standard_encdec_lstm_preds.npz', seed_key)\n",
    "}\n",
    "\n",
    "print('loading conf loss preds')\n",
    "conf_loss_preds = {\n",
    "    'unet': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_unet_preds.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_squeezeformer_preds.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_pure_resLSTM_preds.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_pao_model_preds.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_convnext_preds.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_encdec_lstm_preds.npz', seed_key)\n",
    "}\n",
    "\n",
    "print('loading conf loss conf')\n",
    "conf_loss_conf = {\n",
    "    'unet': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_unet_conf.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_squeezeformer_conf.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_pure_resLSTM_conf.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_pao_model_conf.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_convnext_conf.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(conf_loss_save_path, 'conf_loss_encdec_lstm_conf.npz', seed_key)\n",
    "}\n",
    "\n",
    "print('loading diff loss preds')\n",
    "diff_loss_preds = {\n",
    "    'unet': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_unet_preds.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_squeezeformer_preds.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_pure_resLSTM_preds.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_pao_model_preds.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_convnext_preds.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(diff_loss_save_path, 'diff_loss_encdec_lstm_preds.npz', seed_key)\n",
    "}\n",
    "\n",
    "print('loading multirep preds')\n",
    "multirep_preds = {\n",
    "    'unet': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_unet_preds.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_squeezeformer_preds.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_pure_resLSTM_preds.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_pao_model_preds.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_convnext_preds.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(multirep_save_path, 'multirep_encdec_lstm_preds.npz', seed_key)\n",
    "}\n",
    "\n",
    "print('loading v6 preds')\n",
    "v6_preds = {\n",
    "    'unet': lambda seed_key: load_seed_data(v6_save_path, 'v6_unet_preds.npz', seed_key),\n",
    "    'squeezeformer': lambda seed_key: load_seed_data(v6_save_path, 'v6_squeezeformer_preds.npz', seed_key),\n",
    "    'pure_resLSTM': lambda seed_key: load_seed_data(v6_save_path, 'v6_pure_resLSTM_preds.npz', seed_key),\n",
    "    'pao_model': lambda seed_key: load_seed_data(v6_save_path, 'v6_pao_model_preds.npz', seed_key),\n",
    "    'convnext': lambda seed_key: load_seed_data(v6_save_path, 'v6_convnext_preds.npz', seed_key),\n",
    "    'encdec_lstm': lambda seed_key: load_seed_data(v6_save_path, 'v6_encdec_lstm_preds.npz', seed_key)\n",
    "}\n",
    "\n",
    "config_preds = {\n",
    "    'standard': standard_preds,\n",
    "    'conf_loss': conf_loss_preds,\n",
    "    'diff_loss': diff_loss_preds,\n",
    "    'multirep': multirep_preds,\n",
    "    'v6': v6_preds\n",
    "}\n",
    "\n",
    "print('loading Kaggle preds')\n",
    "kaggle_check_path = '/pscratch/sd/j/jerrylin/kaggle_check'\n",
    "\n",
    "greysnow = np.load(os.path.join(kaggle_check_path, 'prds.npy'))\n",
    "greysnow = greysnow.reshape(-1, data_v2_rh_mc.num_latlon, 368)\n",
    "\n",
    "adam = pd.read_parquet(os.path.join(kaggle_check_path, 'final_blend_v10.parquet'))\n",
    "adam = adam.iloc[:,1:].values\n",
    "adam = adam.reshape(-1, data_v2_rh_mc.num_latlon, 368)\n",
    "\n",
    "print(greysnow.shape, adam.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b77f6",
   "metadata": {},
   "source": [
    "# Load Offline R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e5242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(standard_save_path, \"standard_unet_r2.pkl\"), \"rb\") as f:\n",
    "    standard_unet_r2 = pickle.load(f)\n",
    "with open(os.path.join(standard_save_path, \"standard_squeezeformer_r2.pkl\"), \"rb\") as f:\n",
    "    standard_squeezeformer_r2 = pickle.load(f)\n",
    "with open(os.path.join(standard_save_path, \"standard_pure_resLSTM_r2.pkl\"), \"rb\") as f:\n",
    "    standard_pure_resLSTM_r2 = pickle.load(f)\n",
    "with open(os.path.join(standard_save_path, \"standard_pao_model_r2.pkl\"), \"rb\") as f:\n",
    "    standard_pao_model_r2 = pickle.load(f)\n",
    "with open(os.path.join(standard_save_path, \"standard_convnext_r2.pkl\"), \"rb\") as f:\n",
    "    standard_convnext_r2 = pickle.load(f)\n",
    "with open(os.path.join(standard_save_path, \"standard_encdec_lstm_r2.pkl\"), \"rb\") as f:\n",
    "    standard_encdec_lstm_r2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_unet_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_unet_r2 = pickle.load(f)\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_squeezeformer_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_squeezeformer_r2 = pickle.load(f)\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_pure_resLSTM_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_pure_resLSTM_r2 = pickle.load(f)\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_pao_model_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_pao_model_r2 = pickle.load(f)\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_convnext_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_convnext_r2 = pickle.load(f)\n",
    "with open(os.path.join(conf_loss_save_path, \"conf_loss_encdec_lstm_r2.pkl\"), \"rb\") as f:\n",
    "    conf_loss_encdec_lstm_r2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_unet_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_unet_r2 = pickle.load(f)\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_squeezeformer_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_squeezeformer_r2 = pickle.load(f)\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_pure_resLSTM_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_pure_resLSTM_r2 = pickle.load(f)\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_pao_model_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_pao_model_r2 = pickle.load(f)\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_convnext_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_convnext_r2 = pickle.load(f)\n",
    "with open(os.path.join(diff_loss_save_path, \"diff_loss_encdec_lstm_r2.pkl\"), \"rb\") as f:\n",
    "    diff_loss_encdec_lstm_r2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(multirep_save_path, \"multirep_unet_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_unet_r2 = pickle.load(f)\n",
    "with open(os.path.join(multirep_save_path, \"multirep_squeezeformer_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_squeezeformer_r2 = pickle.load(f)\n",
    "with open(os.path.join(multirep_save_path, \"multirep_pure_resLSTM_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_pure_resLSTM_r2 = pickle.load(f)\n",
    "with open(os.path.join(multirep_save_path, \"multirep_pao_model_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_pao_model_r2 = pickle.load(f)\n",
    "with open(os.path.join(multirep_save_path, \"multirep_convnext_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_convnext_r2 = pickle.load(f)\n",
    "with open(os.path.join(multirep_save_path, \"multirep_encdec_lstm_r2.pkl\"), \"rb\") as f:\n",
    "    multirep_encdec_lstm_r2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(v6_save_path, \"v6_unet_r2.pkl\"), \"rb\") as f:\n",
    "    v6_unet_r2 = pickle.load(f)\n",
    "with open(os.path.join(v6_save_path, \"v6_squeezeformer_r2.pkl\"), \"rb\") as f:\n",
    "    v6_squeezeformer_r2 = pickle.load(f)\n",
    "with open(os.path.join(v6_save_path, \"v6_pure_resLSTM_r2.pkl\"), \"rb\") as f:\n",
    "    v6_pure_resLSTM_r2 = pickle.load(f)\n",
    "with open(os.path.join(v6_save_path, \"v6_pao_model_r2.pkl\"), \"rb\") as f:\n",
    "    v6_pao_model_r2 = pickle.load(f)\n",
    "with open(os.path.join(v6_save_path, \"v6_convnext_r2.pkl\"), \"rb\") as f:\n",
    "    v6_convnext_r2 = pickle.load(f)\n",
    "with open(os.path.join(v6_save_path, \"v6_encdec_lstm_r2.pkl\"), \"rb\") as f:\n",
    "    v6_encdec_lstm_r2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6b3a7",
   "metadata": {},
   "source": [
    "# Load online runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(ds1, ds2, total_weight, var, num_years):\n",
    "    months = np.arange(1, num_years * 12 + 1)\n",
    "    # Initialize the RMSE array with NaN values\n",
    "    rmse_per_month = np.full(len(months), np.nan)\n",
    "    if not ds1:\n",
    "        return rmse_per_month\n",
    "\n",
    "    total_weight = get_dp(ds2) * ds2['area'].values[:,None,:]\n",
    "    # Determine the number of months in ds1\n",
    "    num_months = ds1[var].shape[0]\n",
    "    \n",
    "    # Slice total_weight to match the number of months in ds1\n",
    "    total_weight_sliced = total_weight[:num_months, :, :]\n",
    "    \n",
    "    # Compute RMSE for existing months\n",
    "    squared_diff = (ds1[var] - ds2[var]) ** 2\n",
    "    weighted_squared_diff = squared_diff * total_weight_sliced\n",
    "    weighted_sum = weighted_squared_diff.sum(axis=(1, 2))\n",
    "    total_weight_sum = total_weight_sliced.sum(axis=(1, 2))\n",
    "    weighted_mean_squared_diff = weighted_sum / total_weight_sum\n",
    "    rmse_existing_months = np.sqrt(weighted_mean_squared_diff)\n",
    "    \n",
    "    # Fill in the RMSE array with the computed values\n",
    "    rmse_per_month[:num_months] = rmse_existing_months.values\n",
    "    rmse_per_month = rmse_per_month * online_var_settings[var]['scaling']\n",
    "    return rmse_per_month\n",
    "\n",
    "ds_mmf_1_5_year, ds_mmf_2_5_year = read_mmf_online_data(num_years = 5)\n",
    "ds_mmf_1_4_year, ds_mmf_2_4_year = read_mmf_online_data(num_years = 4)\n",
    "mmf_1_total_weight_5_year = get_pressure_area_weights(ds_mmf_1_5_year)\n",
    "mmf_1_total_weight_4_year = get_pressure_area_weights(ds_mmf_1_4_year)\n",
    "mmf_1_5_year_rmse = {\n",
    "    'T': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='T', num_years=5),\n",
    "    'Q': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='Q', num_years=5),\n",
    "    'U': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='U', num_years=5),\n",
    "    'V': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='V', num_years=5),\n",
    "    'CLDLIQ': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='CLDLIQ', num_years=5),\n",
    "    'CLDICE': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='CLDICE', num_years=5),\n",
    "    'DTPHYS': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='DTPHYS', num_years=5),\n",
    "    'DQ1PHYS': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='DQ1PHYS', num_years=5),\n",
    "    'DQ2PHYS': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='DQ2PHYS', num_years=5),\n",
    "    'DQ3PHYS': calculate_rmse(ds_mmf_2_5_year, ds_mmf_1_5_year, mmf_1_total_weight_5_year, var='DQ3PHYS', num_years=5),\n",
    "}\n",
    "mmf_1_4_year_rmse = {\n",
    "    'T': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='T', num_years=4),\n",
    "    'Q': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='Q', num_years=4),\n",
    "    'U': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='U', num_years=4),\n",
    "    'V': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='V', num_years=4),\n",
    "    'CLDLIQ': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='CLDLIQ', num_years=4),\n",
    "    'CLDICE': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='CLDICE', num_years=4),\n",
    "    'DTPHYS': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='DTPHYS', num_years=4),\n",
    "    'DQ1PHYS': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='DQ1PHYS', num_years=4),\n",
    "    'DQ2PHYS': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='DQ2PHYS', num_years=4),\n",
    "    'DQ3PHYS': calculate_rmse(ds_mmf_2_4_year, ds_mmf_1_4_year, mmf_1_total_weight_4_year, var='DQ3PHYS', num_years=4),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94204f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nn_standard_5_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('standard', 'unet', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('standard', 'squeezeformer', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('standard', 'pure_resLSTM', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('standard', 'pao_model', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('standard', 'convnext', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('standard', 'encdec_lstm', seed_number, num_years = 5) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_conf_loss_5_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('conf_loss', 'unet', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('conf_loss', 'squeezeformer', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('conf_loss', 'pure_resLSTM', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('conf_loss', 'pao_model', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('conf_loss', 'convnext', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('conf_loss', 'encdec_lstm', seed_number, num_years = 5) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_diff_loss_5_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('diff_loss', 'unet', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('diff_loss', 'squeezeformer', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('diff_loss', 'pure_resLSTM', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('diff_loss', 'pao_model', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('diff_loss', 'convnext', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('diff_loss', 'encdec_lstm', seed_number, num_years = 5) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_multirep_5_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('multirep', 'unet', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('multirep', 'squeezeformer', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('multirep', 'pure_resLSTM', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('multirep', 'pao_model', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('multirep', 'convnext', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('multirep', 'encdec_lstm', seed_number, num_years = 5) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_v6_5_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('v6', 'unet', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('v6', 'squeezeformer', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('v6', 'pure_resLSTM', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('v6', 'pao_model', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('v6', 'convnext', seed_number, num_years = 5) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('v6', 'encdec_lstm', seed_number, num_years = 5) for seed_number in seed_numbers}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b96dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nn_standard_4_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('standard', 'unet', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('standard', 'squeezeformer', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('standard', 'pure_resLSTM', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('standard', 'pao_model', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('standard', 'convnext', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('standard', 'encdec_lstm', seed_number, num_years = 4) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_conf_loss_4_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('conf_loss', 'unet', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('conf_loss', 'squeezeformer', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('conf_loss', 'pure_resLSTM', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('conf_loss', 'pao_model', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('conf_loss', 'convnext', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('conf_loss', 'encdec_lstm', seed_number, num_years = 4) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_diff_loss_4_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('diff_loss', 'unet', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('diff_loss', 'squeezeformer', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('diff_loss', 'pure_resLSTM', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('diff_loss', 'pao_model', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('diff_loss', 'convnext', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('diff_loss', 'encdec_lstm', seed_number, num_years = 4) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_multirep_4_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('multirep', 'unet', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('multirep', 'squeezeformer', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('multirep', 'pure_resLSTM', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('multirep', 'pao_model', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('multirep', 'convnext', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('multirep', 'encdec_lstm', seed_number, num_years = 4) for seed_number in seed_numbers}\n",
    "}\n",
    "ds_nn_v6_4_year = {\n",
    "    'unet': {seed_number: read_nn_online_data('v6', 'unet', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'squeezeformer': {seed_number: read_nn_online_data('v6', 'squeezeformer', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pure_resLSTM': {seed_number: read_nn_online_data('v6', 'pure_resLSTM', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'pao_model': {seed_number: read_nn_online_data('v6', 'pao_model', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'convnext': {seed_number: read_nn_online_data('v6', 'convnext', seed_number, num_years = 4) for seed_number in seed_numbers},\n",
    "    'encdec_lstm': {seed_number: read_nn_online_data('v6', 'encdec_lstm', seed_number, num_years = 4) for seed_number in seed_numbers}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_nn_data_save_path = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online/online_nn_data'\n",
    "\n",
    "file_name = \"ds_nn_standard_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_standard_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_conf_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_conf_loss_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_diff_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_diff_loss_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_multirep_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_multirep_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_v6_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_v6_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_standard_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_standard_4_year, file)\n",
    "\n",
    "file_name = \"ds_nn_conf_loss_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_conf_loss_4_year, file)\n",
    "\n",
    "file_name = \"ds_nn_diff_loss_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_diff_loss_4_year, file)\n",
    "\n",
    "file_name = \"ds_nn_multirep_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_multirep_4_year, file)\n",
    "\n",
    "file_name = \"ds_nn_v6_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_data_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_v6_4_year, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nn_rmse_standard_5_year = {\n",
    "    'T':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'T', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'Q':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'Q', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'U':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'U', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'V':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'V', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDLIQ':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDLIQ', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDICE':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDICE', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DTPHYS':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DTPHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ1PHYS':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ1PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ2PHYS':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ2PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ3PHYS':{model_name: np.array([calculate_rmse(ds_nn_standard_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ3PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "}\n",
    "ds_nn_rmse_conf_loss_5_year = {\n",
    "    'T':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'T', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'Q':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'Q', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'U':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'U', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'V':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'V', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDLIQ':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDLIQ', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDICE':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDICE', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DTPHYS':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DTPHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ1PHYS':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ1PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ2PHYS':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ2PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ3PHYS':{model_name: np.array([calculate_rmse(ds_nn_conf_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ3PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "}\n",
    "ds_nn_rmse_diff_loss_5_year = {\n",
    "    'T':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'T', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'Q':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'Q', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'U':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'U', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'V':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'V', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDLIQ':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDLIQ', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDICE':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDICE', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DTPHYS':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DTPHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ1PHYS':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ1PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ2PHYS':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ2PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ3PHYS':{model_name: np.array([calculate_rmse(ds_nn_diff_loss_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ3PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "}\n",
    "ds_nn_rmse_multirep_5_year = {\n",
    "    'T':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'T', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'Q':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'Q', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'U':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'U', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'V':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'V', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDLIQ':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDLIQ', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDICE':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'CLDICE', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DTPHYS':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DTPHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ1PHYS':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ1PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ2PHYS':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ2PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ3PHYS':{model_name: np.array([calculate_rmse(ds_nn_multirep_5_year[model_name][seed_number], ds_mmf_1_5_year, mmf_1_total_weight_5_year, var = 'DQ3PHYS', num_years = 5) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "}\n",
    "ds_nn_rmse_v6_4_year = {\n",
    "    'T':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'T', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'Q':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'Q', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'U':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'U', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'V':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'V', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDLIQ':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'CLDLIQ', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'CLDICE':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'CLDICE', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DTPHYS':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'DTPHYS', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ1PHYS':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'DQ1PHYS', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ2PHYS':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'DQ2PHYS', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "    'DQ3PHYS':{model_name: np.array([calculate_rmse(ds_nn_v6_4_year[model_name][seed_number], ds_mmf_1_4_year, mmf_1_total_weight_4_year, var = 'DQ3PHYS', num_years = 4) for seed_number in seed_numbers]) for model_name in model_names.keys()},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_rmse_growth_save_path = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online/online_rmse_growth'\n",
    "\n",
    "file_name = \"mmf_1_5_year_rmse.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(mmf_1_5_year_rmse, file)\n",
    "\n",
    "file_name = \"mmf_1_4_year_rmse.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(mmf_1_4_year_rmse, file)\n",
    "\n",
    "file_name = \"ds_nn_rmse_standard_5_year.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_rmse_standard_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_rmse_conf_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_rmse_conf_loss_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_rmse_diff_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_rmse_diff_loss_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_rmse_multirep_5_year.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_rmse_multirep_5_year, file)\n",
    "\n",
    "file_name = \"ds_nn_rmse_v6_4_year.pkl\"\n",
    "with open(os.path.join(online_rmse_growth_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(ds_nn_rmse_v6_4_year, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmf_1_hourly_prect_4_year = xr.open_mfdataset('/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data_hourly/precip_hourly/mmf_ref/PRECT*nc')['PRECT'].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000\n",
    "mmf_1_hourly_prect_4_year_flat = mmf_1_hourly_prect_4_year.flatten()\n",
    "\n",
    "nn_hourly_prect_standard_5_year = {model_name: {seed_number: read_nn_online_precip_data('standard', model_name, seed_number, num_years = 5) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_standard_5_year = {model_name: {seed_number: nn_hourly_prect_standard_5_year[model_name][seed_number].sel(time = slice(None, str(5 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_standard_5_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_standard_5_year = {model_name: {seed_number: nn_hourly_prect_standard_5_year[model_name][seed_number] if len(nn_hourly_prect_standard_5_year[model_name][seed_number]) == 365 * 24 * 5 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_conf_loss_5_year = {model_name: {seed_number: read_nn_online_precip_data('conf_loss', model_name, seed_number, num_years = 5) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_conf_loss_5_year = {model_name: {seed_number: nn_hourly_prect_conf_loss_5_year[model_name][seed_number].sel(time = slice(None, str(5 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_conf_loss_5_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_conf_loss_5_year = {model_name: {seed_number: nn_hourly_prect_conf_loss_5_year[model_name][seed_number] if len(nn_hourly_prect_conf_loss_5_year[model_name][seed_number]) == 365 * 24 * 5 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_diff_loss_5_year = {model_name: {seed_number: read_nn_online_precip_data('diff_loss', model_name, seed_number, num_years = 5) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_diff_loss_5_year = {model_name: {seed_number: nn_hourly_prect_diff_loss_5_year[model_name][seed_number].sel(time = slice(None, str(5 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_diff_loss_5_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_diff_loss_5_year = {model_name: {seed_number: nn_hourly_prect_diff_loss_5_year[model_name][seed_number] if len(nn_hourly_prect_diff_loss_5_year[model_name][seed_number]) == 365 * 24 * 5 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_multirep_5_year = {model_name: {seed_number: read_nn_online_precip_data('multirep', model_name, seed_number, num_years = 5) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_multirep_5_year = {model_name: {seed_number: nn_hourly_prect_multirep_5_year[model_name][seed_number].sel(time = slice(None, str(5 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_multirep_5_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_multirep_5_year = {model_name: {seed_number: nn_hourly_prect_multirep_5_year[model_name][seed_number] if len(nn_hourly_prect_multirep_5_year[model_name][seed_number]) == 365 * 24 * 5 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_v6_5_year = {model_name: {seed_number: read_nn_online_precip_data('v6', model_name, seed_number, num_years = 5) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_v6_5_year = {model_name: {seed_number: nn_hourly_prect_v6_5_year[model_name][seed_number].sel(time = slice(None, str(5 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_v6_5_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_v6_5_year = {model_name: {seed_number: nn_hourly_prect_v6_5_year[model_name][seed_number] if len(nn_hourly_prect_v6_5_year[model_name][seed_number]) == 365 * 24 * 5 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_standard_4_year = {model_name: {seed_number: read_nn_online_precip_data('standard', model_name, seed_number, num_years = 4) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_standard_4_year = {model_name: {seed_number: nn_hourly_prect_standard_4_year[model_name][seed_number].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_standard_4_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_standard_4_year = {model_name: {seed_number: nn_hourly_prect_standard_4_year[model_name][seed_number] if len(nn_hourly_prect_standard_4_year[model_name][seed_number]) == 365 * 24 * 4 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_conf_loss_4_year = {model_name: {seed_number: read_nn_online_precip_data('conf_loss', model_name, seed_number, num_years = 4) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_conf_loss_4_year = {model_name: {seed_number: nn_hourly_prect_conf_loss_4_year[model_name][seed_number].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_conf_loss_4_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_conf_loss_4_year = {model_name: {seed_number: nn_hourly_prect_conf_loss_4_year[model_name][seed_number] if len(nn_hourly_prect_conf_loss_4_year[model_name][seed_number]) == 365 * 24 * 4 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_diff_loss_4_year = {model_name: {seed_number: read_nn_online_precip_data('diff_loss', model_name, seed_number, num_years = 4) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_diff_loss_4_year = {model_name: {seed_number: nn_hourly_prect_diff_loss_4_year[model_name][seed_number].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_diff_loss_4_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_diff_loss_4_year = {model_name: {seed_number: nn_hourly_prect_diff_loss_4_year[model_name][seed_number] if len(nn_hourly_prect_diff_loss_4_year[model_name][seed_number]) == 365 * 24 * 4 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_multirep_4_year = {model_name: {seed_number: read_nn_online_precip_data('multirep', model_name, seed_number, num_years = 4) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_multirep_4_year = {model_name: {seed_number: nn_hourly_prect_multirep_4_year[model_name][seed_number].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_multirep_4_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_multirep_4_year = {model_name: {seed_number: nn_hourly_prect_multirep_4_year[model_name][seed_number] if len(nn_hourly_prect_multirep_4_year[model_name][seed_number]) == 365 * 24 * 4 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "\n",
    "nn_hourly_prect_v6_4_year = {model_name: {seed_number: read_nn_online_precip_data('v6', model_name, seed_number, num_years = 4) for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_v6_4_year = {model_name: {seed_number: nn_hourly_prect_v6_4_year[model_name][seed_number].sel(time = slice(None, str(4 + 2).zfill(4))).values * 86400 * 1000 if nn_hourly_prect_v6_4_year[model_name][seed_number] is not None else []\n",
    "                                for seed_number in seed_numbers} for model_name in model_names.keys()}\n",
    "nn_hourly_prect_v6_4_year = {model_name: {seed_number: nn_hourly_prect_v6_4_year[model_name][seed_number] if len(nn_hourly_prect_v6_4_year[model_name][seed_number]) == 365 * 24 * 4 else []\n",
    "                                    for seed_number in seed_numbers} for model_name in model_names.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_nn_hourly_prect_save_path = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online/hourly_prect'\n",
    "\n",
    "file_name = \"hourly_prect_standard_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_standard_5_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_conf_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_conf_loss_5_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_diff_loss_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_diff_loss_5_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_multirep_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_multirep_5_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_v6_5_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_v6_5_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_standard_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_standard_4_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_conf_loss_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_conf_loss_4_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_diff_loss_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_diff_loss_4_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_multirep_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_multirep_4_year, file)\n",
    "\n",
    "file_name = \"hourly_prect_v6_4_year.pkl\"\n",
    "with open(os.path.join(online_nn_hourly_prect_save_path, file_name), \"wb\") as file:\n",
    "    pickle.dump(nn_hourly_prect_v6_4_year, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e8a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating 5 year standard global rmse\n",
      "calculating 5 year conf_loss global rmse\n",
      "calculating 5 year diff_loss global rmse\n",
      "calculating 5 year multirep global rmse\n",
      "calculating 5 year v6 global rmse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_7/run/squeezeformer_v6_seed_7.eam.h0.000[34567]*.nc': No such file or directory\n",
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_43/run/squeezeformer_v6_seed_43.eam.h0.000[34567]*.nc': No such file or directory\n",
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_1024/run/squeezeformer_v6_seed_1024.eam.h0.000[34567]*.nc': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating 4 year standard global rmse\n",
      "calculating 4 year conf_loss global rmse\n",
      "calculating 4 year diff_loss global rmse\n",
      "calculating 4 year multirep global rmse\n",
      "calculating 4 year v6 global rmse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_7/run/squeezeformer_v6_seed_7.eam.h0.000[3456]*.nc': No such file or directory\n",
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_43/run/squeezeformer_v6_seed_43.eam.h0.000[3456]*.nc': No such file or directory\n",
      "ls: cannot access '/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/online_runs/climsim3_ensembles_good/v6/five_year_runs/squeezeformer_v6_seed_1024/run/squeezeformer_v6_seed_1024.eam.h0.000[3456]*.nc': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "def get_global_rmse(config_name, num_years):\n",
    "    months = np.arange(1, num_years * 12 + 1)\n",
    "    ds_mmf_1, ds_mmf_2 = read_mmf_online_data(num_years)\n",
    "    mmf_1_total_weight = get_pressure_area_weights(ds_mmf_1)\n",
    "    ds_nn = {\n",
    "        'unet': {seed_number: read_nn_online_data(config_name, 'unet', seed_number, num_years) for seed_number in seed_numbers},\n",
    "        'squeezeformer': {seed_number: read_nn_online_data(config_name, 'squeezeformer', seed_number, num_years) for seed_number in seed_numbers},\n",
    "        'pure_resLSTM': {seed_number: read_nn_online_data(config_name, 'pure_resLSTM', seed_number, num_years) for seed_number in seed_numbers},\n",
    "        'pao_model': {seed_number: read_nn_online_data(config_name, 'pao_model', seed_number, num_years) for seed_number in seed_numbers},\n",
    "        'convnext': {seed_number: read_nn_online_data(config_name, 'convnext', seed_number, num_years) for seed_number in seed_numbers},\n",
    "        'encdec_lstm': {seed_number: read_nn_online_data(config_name, 'encdec_lstm', seed_number, num_years) for seed_number in seed_numbers}\n",
    "    }\n",
    "    variables = ['T', 'Q', 'U', 'V', 'CLDLIQ', 'CLDICE', 'DTPHYS', 'DQ1PHYS', 'DQnPHYS', 'DUPHYS']\n",
    "    def load_nn_var_time_mean(ds_nn_xr, var, num_years):\n",
    "        return_vals = np.full((data_v2_rh_mc.num_levels, data_v2_rh_mc.num_latlon), np.nan)\n",
    "        if not ds_nn_xr or len(ds_nn_xr['time']) < num_years * 12:\n",
    "            return return_vals\n",
    "        else:\n",
    "            return ds_nn_xr[var].mean(dim = 'time').values\n",
    "    ds_nn_rmse_global_dict = {}\n",
    "    for var in variables:\n",
    "        ds_mmf_1_mean = ds_mmf_1[var].mean(dim = 'time').values * online_var_settings[var]['scaling']\n",
    "        ds_nn_rmse_global_dict[var] = {\n",
    "            'unet': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['unet'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers]),\n",
    "            'squeezeformer': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['squeezeformer'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers]),\n",
    "            'pure_resLSTM': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['pure_resLSTM'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers]),\n",
    "            'pao_model': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['pao_model'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers]),\n",
    "            'convnext': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['convnext'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers]),\n",
    "            'encdec_lstm': np.array([np.sqrt(np.average((load_nn_var_time_mean(ds_nn['encdec_lstm'][seed_number], var, num_years) * online_var_settings[var]['scaling'] - ds_mmf_1_mean) ** 2, weights = mmf_1_total_weight)) for seed_number in seed_numbers])\n",
    "        }\n",
    "    return ds_nn_rmse_global_dict\n",
    "\n",
    "print('calculating 5 year standard global rmse')\n",
    "standard_global_rmse_5_year = get_global_rmse('standard', 5)\n",
    "print('calculating 5 year conf_loss global rmse')\n",
    "conf_loss_global_rmse_5_year = get_global_rmse('conf_loss', 5)\n",
    "print('calculating 5 year diff_loss global rmse')\n",
    "diff_loss_global_rmse_5_year = get_global_rmse('diff_loss', 5)\n",
    "print('calculating 5 year multirep global rmse')\n",
    "multirep_global_rmse_5_year = get_global_rmse('multirep', 5)\n",
    "print('calculating 5 year v6 global rmse')\n",
    "v6_global_rmse_5_year = get_global_rmse('v6', 5)\n",
    "\n",
    "print('calculating 4 year standard global rmse')\n",
    "standard_global_rmse_4_year = get_global_rmse('standard', 4)\n",
    "print('calculating 4 year conf_loss global rmse')\n",
    "conf_loss_global_rmse_4_year = get_global_rmse('conf_loss', 4)\n",
    "print('calculating 4 year diff_loss global rmse')\n",
    "diff_loss_global_rmse_4_year = get_global_rmse('diff_loss', 4)\n",
    "print('calculating 4 year multirep global rmse')\n",
    "multirep_global_rmse_4_year = get_global_rmse('multirep', 4)\n",
    "print('calculating 4 year v6 global rmse')\n",
    "v6_global_rmse_4_year = get_global_rmse('v6', 4)\n",
    "\n",
    "dict_save_path = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online/online_global_rmse_config_dicts'\n",
    "\n",
    "dict_file_path_standard_5_year = os.path.join(dict_save_path, 'standard_global_rmse_5_year.pkl')\n",
    "dict_file_path_conf_loss_5_year = os.path.join(dict_save_path, 'conf_loss_global_rmse_5_year.pkl')\n",
    "dict_file_path_diff_loss_5_year = os.path.join(dict_save_path, 'diff_loss_global_rmse_5_year.pkl')\n",
    "dict_file_path_multirep_5_year = os.path.join(dict_save_path, 'multirep_global_rmse_5_year.pkl')\n",
    "dict_file_path_v6_5_year = os.path.join(dict_save_path, 'v6_global_rmse_5_year.pkl')\n",
    "\n",
    "with open(dict_file_path_standard_5_year, \"wb\") as file:\n",
    "    pickle.dump(standard_global_rmse_5_year, file)\n",
    "\n",
    "with open(dict_file_path_conf_loss_5_year, \"wb\") as file:\n",
    "    pickle.dump(conf_loss_global_rmse_5_year, file)\n",
    "\n",
    "with open(dict_file_path_diff_loss_5_year, \"wb\") as file:\n",
    "    pickle.dump(diff_loss_global_rmse_5_year, file)\n",
    "\n",
    "with open(dict_file_path_multirep_5_year, \"wb\") as file:\n",
    "    pickle.dump(multirep_global_rmse_5_year, file)\n",
    "\n",
    "with open(dict_file_path_v6_5_year, \"wb\") as file:\n",
    "    pickle.dump(v6_global_rmse_5_year, file)\n",
    "\n",
    "dict_file_path_standard_4_year = os.path.join(dict_save_path, 'standard_global_rmse_4_year.pkl')\n",
    "dict_file_path_conf_loss_4_year = os.path.join(dict_save_path, 'conf_loss_global_rmse_4_year.pkl')\n",
    "dict_file_path_diff_loss_4_year = os.path.join(dict_save_path, 'diff_loss_global_rmse_4_year.pkl')\n",
    "dict_file_path_multirep_4_year = os.path.join(dict_save_path, 'multirep_global_rmse_4_year.pkl')\n",
    "dict_file_path_v6_4_year = os.path.join(dict_save_path, 'v6_global_rmse_4_year.pkl')\n",
    "\n",
    "with open(dict_file_path_standard_4_year, \"wb\") as file:\n",
    "    pickle.dump(standard_global_rmse_4_year, file)\n",
    "\n",
    "with open(dict_file_path_conf_loss_4_year, \"wb\") as file:\n",
    "    pickle.dump(conf_loss_global_rmse_4_year, file)\n",
    "\n",
    "with open(dict_file_path_diff_loss_4_year, \"wb\") as file:\n",
    "    pickle.dump(diff_loss_global_rmse_4_year, file)\n",
    "\n",
    "with open(dict_file_path_multirep_4_year, \"wb\") as file:\n",
    "    pickle.dump(multirep_global_rmse_4_year, file)\n",
    "\n",
    "with open(dict_file_path_v6_4_year, \"wb\") as file:\n",
    "    pickle.dump(v6_global_rmse_4_year, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f4cbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_global_rmse_dict_5_year = {\n",
    "    'standard': standard_global_rmse_5_year,\n",
    "    'conf_loss': conf_loss_global_rmse_5_year,\n",
    "    'diff_loss': diff_loss_global_rmse_5_year,\n",
    "    'multirep': multirep_global_rmse_5_year,\n",
    "    'v6': v6_global_rmse_5_year\n",
    "}\n",
    "\n",
    "global_rmse_dict_full_5_year = {'T': [], 'Q': [], 'U': [], 'V': [], 'CLDLIQ': [], 'CLDICE': [], 'DTPHYS': [], 'DQ1PHYS': [], 'DQnPHYS': [], 'DUPHYS': []}\n",
    "for var in global_rmse_dict_full_5_year.keys():\n",
    "    for config_name in config_names.keys():\n",
    "        for model_name in model_names.keys():\n",
    "            for idx, seed_number in enumerate(seed_numbers):\n",
    "                new_result = nn_global_rmse_dict_5_year[config_name][var][model_name][idx]\n",
    "                if not np.isnan(new_result):\n",
    "                    global_rmse_dict_full_5_year[var].append({'config_name': config_name, \n",
    "                                                    'model_name': model_name,\n",
    "                                                    'seed_idx': idx, \n",
    "                                                    'seed_number': seed_number,\n",
    "                                                    'rmse': new_result})\n",
    "\n",
    "nn_global_rmse_dict_4_year = {\n",
    "    'standard': standard_global_rmse_4_year,\n",
    "    'conf_loss': conf_loss_global_rmse_4_year,\n",
    "    'diff_loss': diff_loss_global_rmse_4_year,\n",
    "    'multirep': multirep_global_rmse_4_year,\n",
    "    'v6': v6_global_rmse_4_year\n",
    "}\n",
    "\n",
    "global_rmse_dict_full_4_year = {'T': [], 'Q': [], 'U': [], 'V': [], 'CLDLIQ': [], 'CLDICE': [], 'DTPHYS': [], 'DQ1PHYS': [], 'DQnPHYS': [], 'DUPHYS': []}\n",
    "for var in global_rmse_dict_full_4_year.keys():\n",
    "    for config_name in config_names.keys():\n",
    "        for model_name in model_names.keys():\n",
    "            for idx, seed_number in enumerate(seed_numbers):\n",
    "                new_result = nn_global_rmse_dict_4_year[config_name][var][model_name][idx]\n",
    "                if not np.isnan(new_result):\n",
    "                    global_rmse_dict_full_4_year[var].append({'config_name': config_name, \n",
    "                                                    'model_name': model_name,\n",
    "                                                    'seed_idx': idx, \n",
    "                                                    'seed_number': seed_number,\n",
    "                                                    'rmse': new_result})\n",
    "\n",
    "top_model_dict_5_year = {}\n",
    "for var in global_rmse_dict_full_5_year.keys():\n",
    "    global_rmse_dict_full_5_year[var] = sorted(global_rmse_dict_full_5_year[var], key = lambda sota_dict: sota_dict['rmse'])\n",
    "    top_model_dict_5_year[var] =  global_rmse_dict_full_5_year[var][0]\n",
    "\n",
    "top_model_dict_4_year = {}\n",
    "for var in global_rmse_dict_full_4_year.keys():\n",
    "    global_rmse_dict_full_4_year[var] = sorted(global_rmse_dict_full_4_year[var], key = lambda sota_dict: sota_dict['rmse'])\n",
    "    top_model_dict_4_year[var] =  global_rmse_dict_full_4_year[var][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5dd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_save_path = '/global/cfs/cdirs/m4334/jerry/climsim3_figures/online/online_global_rmse_config_dicts'\n",
    "dict_file_path_standard = os.path.join(dict_save_path, 'standard_global_rmse.pkl')\n",
    "dict_file_path_conf_loss = os.path.join(dict_save_path, 'conf_loss_global_rmse.pkl')\n",
    "dict_file_path_diff_loss = os.path.join(dict_save_path, 'diff_loss_global_rmse.pkl')\n",
    "dict_file_path_multirep = os.path.join(dict_save_path, 'multirep_global_rmse.pkl')\n",
    "dict_file_path_v6 = os.path.join(dict_save_path, 'v6_global_rmse.pkl')\n",
    "\n",
    "with open(dict_file_path_standard, \"rb\") as file:\n",
    "    standard_global_rmse = pickle.load(file)\n",
    "\n",
    "with open(dict_file_path_conf_loss, \"rb\") as file:\n",
    "    conf_loss_global_rmse = pickle.load(file)\n",
    "\n",
    "with open(dict_file_path_diff_loss, \"rb\") as file:\n",
    "    diff_loss_global_rmse = pickle.load(file)\n",
    "\n",
    "with open(dict_file_path_multirep, \"rb\") as file:\n",
    "    multirep_global_rmse = pickle.load(file)\n",
    "\n",
    "with open(dict_file_path_v6, \"rb\") as file:\n",
    "    v6_global_rmse = pickle.load(file)\n",
    "\n",
    "nn_global_rmse_dict = {\n",
    "    'standard': standard_global_rmse,\n",
    "    'conf_loss': conf_loss_global_rmse,\n",
    "    'diff_loss': diff_loss_global_rmse,\n",
    "    'multirep': multirep_global_rmse,\n",
    "    'v6': v6_global_rmse\n",
    "}\n",
    "\n",
    "prev_sota = {'T': .98, 'Q': .25, 'CLDLIQ': 5.39, 'CLDICE': 2.09, 'U': 1.68, 'V': .77}\n",
    "sota_breakers = {'T': [], 'Q': [], 'CLDLIQ': [], 'CLDICE': [], 'U': [], 'V': []}\n",
    "nonnan_rmse_dict = {'T': [], 'Q': [], 'CLDLIQ': [], 'CLDICE': [], 'U': [], 'V': []}\n",
    "for var in prev_sota.keys():\n",
    "    for config_name in config_names.keys():\n",
    "        for model_name in model_names.keys():\n",
    "            for idx, seed_number in enumerate(seed_numbers):\n",
    "                new_result = np.round(nn_global_rmse_dict[config_name][var][model_name][idx], 2)\n",
    "                if not np.isnan(new_result):\n",
    "                    nonnan_rmse_dict[var].append({'config_name': config_name, \n",
    "                                                  'model_name': model_name,\n",
    "                                                  'seed_idx': idx, \n",
    "                                                  'seed_number': seed_number,\n",
    "                                                  'rmse': new_result,\n",
    "                                                  'sypd': sypd_dict[config_name][f'{model_name}_{seed_number}'],\n",
    "                                                  'pc': pc_dict[config_name][f'{model_name}_{seed_number}']})\n",
    "                    if new_result < prev_sota[var]:\n",
    "                        sota_breakers[var].append({'config_name': config_name, \n",
    "                                                   'model_name': model_name,\n",
    "                                                   'seed_idx': idx, \n",
    "                                                   'seed_number': seed_number,\n",
    "                                                   'rmse': new_result,\n",
    "                                                   'sypd': sypd_dict[config_name][f'{model_name}_{seed_number}'],\n",
    "                                                   'pc': pc_dict[config_name][f'{model_name}_{seed_number}']})\n",
    "\n",
    "for var in prev_sota.keys():\n",
    "    sota_breakers[var] = sorted(sota_breakers[var], key = lambda sota_dict: sota_dict['rmse'])\n",
    "    nonnan_rmse_dict[var] = sorted(nonnan_rmse_dict[var], key = lambda nonnan_dict: nonnan_dict['rmse'])\n",
    "\n",
    "zeyuan_path = '/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/unet_v5/'\n",
    "def read_nn_online_data_zeyuan(config_name, num_years):\n",
    "    assert num_years <= 5 and num_years >= 1\n",
    "    years_regexp = '34567'[:num_years]\n",
    "    assert config_name in ['huber_rop', 'huber_step']\n",
    "    if config_name == 'huber_rop':\n",
    "        extract_path = os.path.join(zeyuan_path, config_name, f'v5_noclassifier_huber_1y_noaggressive_rop2_5year_3node.eam.h0.000[{years_regexp}]*.nc')\n",
    "    elif config_name == 'huber_step':\n",
    "        extract_path = os.path.join(zeyuan_path, config_name, f'v5_noclassifier_huber_1y_noaggressive_5year_3node.eam.h0.000[{years_regexp}]*.nc')\n",
    "    ds_nn = xr.open_mfdataset(extract_path)\n",
    "    if len(ds_nn['time']) < 12 * num_years:\n",
    "        return None\n",
    "    ds_nn['DQnPHYS'] = ds_nn['DQ2PHYS'] + ds_nn['DQ3PHYS']\n",
    "    ds_nn['TOTCLD'] = ds_nn['CLDICE'] + ds_nn['CLDLIQ']\n",
    "    ds_nn['PRECT'] = ds_nn['PRECC'] + ds_nn['PRECL']\n",
    "    return ds_nn\n",
    "\n",
    "huber_rop_run = read_nn_online_data_zeyuan('huber_rop', 5)\n",
    "huber_step_run = read_nn_online_data_zeyuan('huber_step', 5)\n",
    "\n",
    "huetal_sota_dict = {\n",
    "    'T': (.99, 1.21),\n",
    "    'Q': (.33, .25),\n",
    "    'CLDLIQ': (13.05, 5.40),\n",
    "    'CLDICE': (2.10, 2.29),\n",
    "    'U': (1.70, 1.98),\n",
    "    'V': (.79, .89)\n",
    "}\n",
    "\n",
    "mmf_ref_dict = {\n",
    "    'T': .18,\n",
    "    'Q': .06,\n",
    "    'CLDLIQ': .80,\n",
    "    'CLDICE': .65,\n",
    "    'U': .44,\n",
    "    'V': .35\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
