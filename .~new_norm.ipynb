{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bda8721-19cd-4dd5-89c0-8dc450af5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, glob, string\n",
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63ee412-df61-42ea-a0f3-7d9529365ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lev \u001b[38;5;241m=\u001b[39m grid_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     52\u001b[0m area_weight \u001b[38;5;241m=\u001b[39m grid_area\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(grid_area)\n\u001b[0;32m---> 54\u001b[0m ds_sp \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_ref/control_fullysp_jan_wmlio_r3.eam.h0.0003.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m ds_sp_2 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_b/control_fullysp_jan_wmlio_r3_b.eam.h0.0003.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m sp_dp \u001b[38;5;241m=\u001b[39m get_dp(ds_sp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/api.py:990\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m paths \u001b[38;5;241m=\u001b[39m _find_absolute_paths(paths, engine\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno files to open\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(concat_dim, (\u001b[38;5;28mstr\u001b[39m, DataArray)) \u001b[38;5;129;01mor\u001b[39;00m concat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "grid_info = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/grid_info/ClimSim_low-res_grid-info.nc')\n",
    "input_mean = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_mean_v2_rh_mc_pervar.nc')\n",
    "input_max = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_max_v2_rh_mc_pervar.nc')\n",
    "input_min = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_min_v2_rh_mc_pervar.nc')\n",
    "output_scale = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/outputs/output_scale_std_lowerthred_v6.nc')\n",
    "lbd_qn = np.loadtxt('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/qn_exp_lambda_large.txt', delimiter = ',')\n",
    "\n",
    "data = data_utils(grid_info = grid_info,\n",
    "                input_mean = input_mean, \n",
    "                input_max = input_max, \n",
    "                input_min = input_min, \n",
    "                output_scale = output_scale,\n",
    "                qinput_log = False,\n",
    "                normalize = False)\n",
    "data.set_to_v6_vars()\n",
    "\n",
    "lat = grid_info['lat'].values\n",
    "lon = grid_info['lon'].values\n",
    "lon = grid_info['lon'].values\n",
    "lon = ((lon + 180) % 360) - 180\n",
    "level = grid_info.lev.values\n",
    "lat_bin_mids = data.lat_bin_mids\n",
    "\n",
    "def area_mean(ds, var):\n",
    "    arr = ds[var].values\n",
    "    arr_reshaped = np.transpose(arr, (0,2,1))\n",
    "    arr_zonal_mean = data.zonal_bin_weight_3d(arr_reshaped)\n",
    "    return arr_zonal_mean\n",
    "\n",
    "def zonal_diff(ds_sp, ds_nn, var):\n",
    "    diff_zonal_mean = (area_mean(ds_nn, var) - area_mean(ds_sp, var)).mean(axis = 0)\n",
    "    diff_zonal = xr.DataArray(diff_zonal_mean.T, dims = ['level', 'lat'], coords = {'level':level, 'lat': lat_bin_mids})\n",
    "    return diff_zonal\n",
    "\n",
    "def get_dp(ds):\n",
    "    ps = ds['PS']\n",
    "    p_interface = (ds['hyai'] * ds['P0'] + ds['hybi'] * ds['PS']).values\n",
    "    if p_interface.shape[0] == 61:\n",
    "        p_interface = np.swapaxes(p_interface, 0, 1)\n",
    "    dp = p_interface[:,1:61,:] - p_interface[:,0:60,:]\n",
    "    return dp\n",
    "\n",
    "def get_tcp_mean(ds, area_weight):\n",
    "    cld = (ds['CLDICE'] + ds['CLDLIQ']).values\n",
    "    dp = get_dp(ds)\n",
    "    tcp = np.sum(cld*dp, axis = 1)/9.81\n",
    "    tcp_mean = np.average(tcp, weights = area_weight, axis = 1)\n",
    "    return tcp_mean\n",
    "\n",
    "grid_area = grid_info['area'].values\n",
    "lev = grid_info['lev'].values\n",
    "area_weight = grid_area/np.sum(grid_area)\n",
    "\n",
    "ds_sp = xr.open_mfdataset('/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_ref/control_fullysp_jan_wmlio_r3.eam.h0.0003.nc')\n",
    "ds_sp_2 = xr.open_mfdataset('/pscratch/sd/z/zeyuanhu/hu_etal2024_data_v2/data/h0/5year/mmf_b/control_fullysp_jan_wmlio_r3_b.eam.h0.0003.nc')\n",
    "\n",
    "sp_dp = get_dp(ds_sp)\n",
    "total_weight = sp_dp * area_weight[None, None, :]\n",
    "total_weight = total_weight.mean(axis = 0)\n",
    "total_weight = total_weight/total_weight.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35fd60-dcb4-41d9-917d-9643a1c262aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_unet_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/unet/run/unet.eam.h0.0003-**.nc')\n",
    "ds_squeezeformer_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/squeezeformer/run/squeezeformer.eam.h0.0003-**.nc')\n",
    "ds_pure_resLSTM_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/pure_resLSTM/run/pure_resLSTM.eam.h0.0003-**.nc')\n",
    "ds_pao_model_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/pao_model/run/pao_model.eam.h0.0003-**.nc')\n",
    "ds_convnext_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/convnext/run/convnext.eam.h0.0003-**.nc')\n",
    "ds_encdec_lstm_v2_rh_mc = xr.open_mfdataset('/global/cfs/cdirs/m4334/jerry/prelim_runs/long_runs/encdec_lstm/run/encdec_lstm.eam.h0.0003-**.nc')\n",
    "\n",
    "diff_zonal_unet_v2_rh_mc_T = zonal_diff(ds_sp, ds_unet_v2_rh_mc, 'T')\n",
    "diff_zonal_squeezeformer_v2_rh_mc_T = zonal_diff(ds_sp, ds_squeezeformer_v2_rh_mc, 'T')\n",
    "diff_zonal_pure_resLSTM_v2_rh_mc_T = zonal_diff(ds_sp, ds_pure_resLSTM_v2_rh_mc, 'T')\n",
    "diff_zonal_pao_model_v2_rh_mc_T = zonal_diff(ds_sp, ds_pao_model_v2_rh_mc, 'T')\n",
    "diff_zonal_convnext_v2_rh_mc_T = zonal_diff(ds_sp, ds_convnext_v2_rh_mc, 'T')\n",
    "diff_zonal_encdec_lstm_v2_rh_mc_T = zonal_diff(ds_sp, ds_encdec_lstm_v2_rh_mc, 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4143b0-151e-4f98-ac8a-0a47c85aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3), constrained_layout=True)  # Increased width to accommodate third column\n",
    "gs = fig.add_gridspec(1, 1)  # Changed to 1x1 grid\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "diff_zonal_sp_2_T = zonal_diff(ds_sp, ds_sp_2, 'T')\n",
    "diff_zonal_sp_2_T.plot(ax = ax0, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "ax0.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6c60c-71b5-477c-866f-ea706402fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8), constrained_layout=True)  # Increased width to accommodate third column\n",
    "gs = fig.add_gridspec(2, 3)  # Changed to 1x3 grid\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "diff_zonal_unet_v2_rh_mc_T.plot(ax = ax0, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "diff_zonal_squeezeformer_v2_rh_mc_T.plot(ax = ax1, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "diff_zonal_pure_resLSTM_v2_rh_mc_T.plot(ax = ax2, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "diff_zonal_pao_model_v2_rh_mc_T.plot(ax = ax3, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "diff_zonal_convnext_v2_rh_mc_T.plot(ax = ax4, vmin = -5, vmax = 5, cmap = 'coolwarm')\n",
    "diff_zonal_encdec_lstm_v2_rh_mc_T.plot(ax = ax5, vmin = -5, vmax = 5, cmap = 'coolwarm', cbar_kwargs = {'orientation': 'vertical', 'pad': 0.05, 'shrink': 1})\n",
    "\n",
    "ax0.invert_yaxis()\n",
    "ax1.invert_yaxis()\n",
    "ax2.invert_yaxis()\n",
    "ax3.invert_yaxis()\n",
    "ax4.invert_yaxis()\n",
    "ax5.invert_yaxis()\n",
    "\n",
    "ax0.set_title('U-net')\n",
    "ax1.set_title('squeezeformer')\n",
    "ax2.set_title('pure_resLSTM')\n",
    "ax3.set_title('pao_model')\n",
    "ax4.set_title('convnext')\n",
    "ax5.set_title('encdec_lstm')\n",
    "\n",
    "ax0.set_ylabel('v2_rh_mc variable list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01347d1a-9d02-4e3f-b381-a9e158a11eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtime = np.arange(12)\n",
    "\n",
    "area_weight_nh = np.where(lat > 30, area_weight, 0)\n",
    "area_weight_sh = np.where(lat < -30, area_weight, 0)\n",
    "area_weight_tropics = np.where((lat > - 30) & (lat < 30), area_weight, 0)\n",
    "\n",
    "# Titles for columns\n",
    "column_titles = ['Global mean', '30N-90N mean', '30S-90S mean', '30S-30N mean']\n",
    "\n",
    "# Y-axis labels for rows\n",
    "row_ylabels = [\n",
    "    'T$_{59}$ (K)',\n",
    "    'Precipitable water (kg/m$^2$)',\n",
    "    'Total cloud path (kg/m$^2$)'\n",
    "]\n",
    "\n",
    "# Define area weights\n",
    "weights = ['area_weight', 'area_weight_nh', 'area_weight_sh', 'area_weight_tropics']\n",
    "weights_dict = {\n",
    "    'area_weight': area_weight,\n",
    "    'area_weight_nh': area_weight_nh,\n",
    "    'area_weight_sh': area_weight_sh,\n",
    "    'area_weight_tropics': area_weight_tropics,\n",
    "}\n",
    "\n",
    "# Define variables and associated averaging functions\n",
    "variables = ['T', 'TMQ', 'TCP']\n",
    "get_mean_function = {\n",
    "    'T': lambda ds, w: np.average(ds['T'][:, -1, :].values, weights=w, axis=1),\n",
    "    'TMQ': lambda ds, w: np.average(ds['TMQ'][:, :].values, weights=w, axis=1),\n",
    "    'TCP': lambda ds, w: get_tcp_mean(ds, w)\n",
    "}\n",
    "\n",
    "# Create a figure with 3 rows and 4 columns\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 8))\n",
    "\n",
    "# Generate subplots\n",
    "for row, var in enumerate(variables):\n",
    "    for col, weight_key in enumerate(weights):\n",
    "        weight = weights_dict[weight_key]\n",
    "        mmf_mean = get_mean_function[var](ds_sp, weight)\n",
    "        mmf2_mean = get_mean_function[var](ds_sp_2, weight)\n",
    "        unet_mean = get_mean_function[var](ds_unet_v2_rh_mc, weight)\n",
    "        squeezeformer_mean = get_mean_function[var](ds_squeezeformer_v2_rh_mc, weight)\n",
    "        pure_resLSTM_mean = get_mean_function[var](ds_pure_resLSTM_v2_rh_mc, weight)\n",
    "        pao_model_mean = get_mean_function[var](ds_pao_model_v2_rh_mc, weight)\n",
    "        convnext_mean = get_mean_function[var](ds_convnext_v2_rh_mc, weight)\n",
    "        encdec_lstm_mean = get_mean_function[var](ds_encdec_lstm_v2_rh_mc, weight)\n",
    "        \n",
    "        axes[row, col].plot(xtime, mmf_mean, label='MMF', color='black')\n",
    "        axes[row, col].plot(xtime, mmf2_mean, label='MMF2', color='black', linestyle = 'dashed')\n",
    "        axes[row, col].plot(xtime, unet_mean, label='unet', color='green', linestyle = 'dashed')\n",
    "        axes[row, col].plot(xtime, squeezeformer_mean, label='squeezeformer', color='purple', linestyle = 'dashed')\n",
    "        axes[row, col].plot(xtime, pure_resLSTM_mean, label='pure_resLSTM', color='blue', linestyle = 'dashed')\n",
    "        axes[row, col].plot(xtime, pao_model_mean, label='pao_model', color='red', linestyle = 'dashed')\n",
    "        # axes[row, col].plot(xtime, convnext_mean, label='convnext', color='yellow', linestyle = 'dashed')\n",
    "        axes[row, col].plot(xtime, encdec_lstm_mean, label='encdec_lstm', color='brown', linestyle = 'dashed')\n",
    "        axes[row, col].tick_params(axis='both', labelsize=12) \n",
    "        # Set column titles\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(column_titles[col],fontsize=14)\n",
    "        \n",
    "        # Set row y-labels\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(row_ylabels[row],fontsize=12)\n",
    "        \n",
    "        # Set x-label for the last row\n",
    "        if row == 2:\n",
    "            axes[row, col].set_xlabel(\"Months\",fontsize=12)\n",
    "        \n",
    "        # Add legend to the first row, last column\n",
    "        if row == 2 and col == 0:\n",
    "            axes[row, col].legend(fontsize=10,loc='upper right',ncols=2)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig('time_series_mean_5years.pdf', format='pdf', dpi=400, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6fd43-0e4a-4a0e-962a-5e8907c7485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['T', 'Q', 'CLDLIQ', 'CLDICE', 'U', 'V']\n",
    "scaling_dict = {'T': 1.0, 'Q': 1000.0, 'CLDLIQ': 1e6, 'CLDICE': 1e6, 'U': 1.0, 'V': 1.0}  # Scaling factors\n",
    "units_dict = {'T': 'K', 'Q': 'g/kg', 'CLDLIQ': 'mg/kg', 'CLDICE': 'mg/kg', 'U': 'm/s', 'V': 'm/s'}  # Units\n",
    "titles = [\n",
    "    '(a) T',\n",
    "    '(b) Q',\n",
    "    '(c) Liquid cloud',\n",
    "    '(d) Ice cloud',\n",
    "    '(e) U',\n",
    "    '(f) V',\n",
    "]  # Main titles with subplot labels\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10), sharey=True, constrained_layout=True)  # 2 rows, 3 columns\n",
    "axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "\n",
    "for ax, var, title in zip(axes, variables, titles):\n",
    "    # Use specified scaling for the variable\n",
    "    scaling = scaling_dict[var]\n",
    "\n",
    "    # Compute time-mean and RMSE for each variable\n",
    "    mmf_mean = ds_sp[var].mean(dim='time').values * scaling\n",
    "    mmf2_mean = ds_sp_2[var].mean(dim='time').values * scaling\n",
    "    unet_mean = ds_unet_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    squeezeformer_mean = ds_squeezeformer_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    pure_resLSTM_mean = ds_pure_resLSTM_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    pao_model_mean = ds_pao_model_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    convnext_mean = ds_convnext_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    encdec_lstm_mean = ds_encdec_lstm_v2_rh_mc[var].mean(dim='time').values * scaling\n",
    "    \n",
    "    mmf_rmse = np.sqrt(np.average((mmf2_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    unet_rmse = np.sqrt(np.average((unet_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    squeezeformer_rmse = np.sqrt(np.average((squeezeformer_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    pure_resLSTM_rmse = np.sqrt(np.average((pure_resLSTM_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    pao_model_rmse = np.sqrt(np.average((pao_model_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    convnext_rmse = np.sqrt(np.average((convnext_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "    encdec_lstm_rmse = np.sqrt(np.average((encdec_lstm_mean - mmf_mean) ** 2, axis=1, weights=area_weight))\n",
    "\n",
    "    mmf_rmse_global = np.sqrt(np.average((mmf2_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    unet_rmse_global = np.sqrt(np.average((unet_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    squeezeformer_rmse_global = np.sqrt(np.average((squeezeformer_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    pure_resLSTM_rmse_global = np.sqrt(np.average((pure_resLSTM_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    pao_model_rmse_global = np.sqrt(np.average((pao_model_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    convnext_rmse_global = np.sqrt(np.average((convnext_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "    encdec_lstm_rmse_global = np.sqrt(np.average((encdec_lstm_mean - mmf_mean) ** 2, weights=total_weight))\n",
    "\n",
    "    # Plot on the corresponding axis\n",
    "    ax.plot(mmf_rmse, lev, label=f'Ref \\n{mmf_rmse_global:.2f}', linestyle='--', color='black')\n",
    "    ax.plot(unet_rmse, lev, label=f'unet \\n{unet_rmse_global:.2f}', linestyle='--', color='green')\n",
    "    ax.plot(squeezeformer_rmse, lev, label=f'squeezeformer \\n{squeezeformer_rmse_global:.2f}', linestyle='--', color='purple')\n",
    "    ax.plot(pure_resLSTM_rmse, lev, label=f'pure_resLSTM \\n{pure_resLSTM_rmse_global:.2f}', linestyle='--', color='blue')\n",
    "    ax.plot(pao_model_rmse, lev, label=f'pao_model \\n{pao_model_rmse_global:.2f}', linestyle='--', color='red')\n",
    "    # ax.plot(convnext_rmse, lev, label=f'convnext \\n{convnext_rmse_global:.2f}', linestyle='--', color='yellow')\n",
    "    ax.plot(encdec_lstm_rmse, lev, label=f'encdec_lstm \\n{encdec_lstm_rmse_global:.2f}', linestyle='--', color='brown')\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    ax.set_title(title+f' ({units_dict[var]})', fontsize=14, loc='center')  # Add main title with subplot label\n",
    "    # ax.set_xlabel(f'{units_dict[var]}', fontsize=14)  # Keep unit in x-label\n",
    "    ax.invert_yaxis()  # Reverse the y-axis\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "# Set a shared y-label for the first column\n",
    "axes[0].set_ylabel('Hybrid pressure (hPa)', fontsize=14)\n",
    "axes[3].set_ylabel('Hybrid pressure (hPa)', fontsize=14)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.tight_layout()\n",
    "# plt.savefig('state_rmse_profiles_and_scalar.pdf', format='pdf', dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3163fbf-1d40-4081-bb57-d8602758f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve precomputed 1-year tropopause level distribution (we used as a microphysics constraint)\n",
    "idx_p400_t10 = np.load('/pscratch/sd/z/zeyuanhu/hu_etal2024_data/microphysics_hourly/first_true_indices_p400_t10.npy')\n",
    "for i in range(idx_p400_t10.shape[0]):\n",
    "    for j in range(idx_p400_t10.shape[1]):\n",
    "        idx_p400_t10[i,j] = level[int(idx_p400_t10[i,j])]\n",
    "\n",
    "idx_p400_t10 = idx_p400_t10.mean(axis=0)\n",
    "idx_p400_t10 = idx_p400_t10[:,np.newaxis]\n",
    "\n",
    "idx_tropopause_zm = data.zonal_bin_weight_2d(idx_p400_t10.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a93980-b462-4389-8dd6-2299fd13c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import string\n",
    "\n",
    "\n",
    "# List of variables and their settings\n",
    "variables = [\n",
    "    {'var': 'T', 'var_title': 'T', 'scaling': 1., 'unit': 'K', 'diff_scale': 0.9, 'max_diff': 5},\n",
    "    {'var': 'Q', 'var_title': 'Q', 'scaling': 1000., 'unit': 'g/kg', 'diff_scale': 1, 'max_diff': 1},\n",
    "    {'var': 'U', 'var_title': 'U', 'scaling': 1., 'unit': 'm/s', 'diff_scale': 0.2, 'max_diff': 4},\n",
    "    {'var': 'CLDLIQ', 'var_title': 'Liquid cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'diff_scale': 1, 'max_diff': 40},\n",
    "    {'var': 'CLDICE', 'var_title': 'Ice cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'diff_scale': 1, 'max_diff': 5}\n",
    "]\n",
    "\n",
    "latitude_ticks = [-60, -30, 0, 30, 60]\n",
    "latitude_labels = ['60S', '30S', '0', '30N', '60N']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(5, 3, figsize=(14, 12.5)) \n",
    "# Generate the panel labels\n",
    "labels = [f\"({letter})\" for letter in string.ascii_lowercase[:15]]\n",
    "\n",
    "\n",
    "# Loop through each variable and its corresponding subplot row\n",
    "for idx, var_info in enumerate(variables):\n",
    "    var = var_info['var']\n",
    "    var_title = var_info['var_title']\n",
    "    scaling = var_info['scaling']\n",
    "    unit = var_info['unit']\n",
    "    diff_scale = var_info['diff_scale']\n",
    "\n",
    "    # Compute the means and differences for plots\n",
    "    sp_tmean = ds_sp[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    nn_tmean = ds_nn[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    sp_zm, lats_sorted = zonal_mean_area_weighted(sp_tmean, grid_area, lat)\n",
    "    nn_zm, lats_sorted = zonal_mean_area_weighted(nn_tmean, grid_area, lat)\n",
    "    \n",
    "    \n",
    "    data_sp = scaling * xr.DataArray(sp_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_nn = scaling * xr.DataArray(nn_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_diff = data_nn - data_sp\n",
    "    \n",
    "    # Determine color scales\n",
    "    vmax = max(abs(data_sp).max(), abs(data_nn).max())\n",
    "    vmin = min(abs(data_sp).min(), abs(data_nn).min())\n",
    "    # if var_info['diff_scale']:\n",
    "    #     vmax_diff = abs(data_diff).max() * diff_scale\n",
    "    #     vmin_diff = -vmax_diff\n",
    "    vmax_diff = var_info['max_diff']\n",
    "    vmin_diff = -vmax_diff\n",
    "    # Plot each variable in its row\n",
    "    data_sp.plot(ax=axs[idx, 0], add_colorbar=True, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 0].set_title(f'{labels[idx * 3]} {var_title} ({unit}): MMF')\n",
    "    axs[idx, 0].invert_yaxis()\n",
    "\n",
    "    data_nn.plot(ax=axs[idx, 1], add_colorbar=True, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 1].set_title(f'{labels[idx * 3 + 1]} {var_title} ({unit}): NN')\n",
    "    axs[idx, 1].invert_yaxis()\n",
    "    axs[idx, 1].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "\n",
    "    data_diff.plot(ax=axs[idx, 2], add_colorbar=True, cmap='RdBu_r', vmin=vmin_diff, vmax=vmax_diff)\n",
    "    axs[idx, 2].set_title(f'{labels[idx * 3 + 2]} {var_title} ({unit}): NN - MMF')\n",
    "    axs[idx, 2].invert_yaxis()\n",
    "    axs[idx, 2].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "    \n",
    "    axs[idx, 0].set_xlabel('')\n",
    "    axs[idx, 1].set_xlabel('')\n",
    "    axs[idx, 2].set_xlabel('')\n",
    "\n",
    "tropopause_pressure = idx_tropopause_zm[0].flatten()  # Flatten to 1D array\n",
    "tropopause_latitude = idx_tropopause_zm[1]  # Latitude values\n",
    "axs[4, 2].plot(tropopause_latitude, tropopause_pressure, 'k--')\n",
    "\n",
    "# Set these ticks and labels for each subplot\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xticks(latitude_ticks)  # Set the positions for the ticks\n",
    "        ax.set_xticklabels(latitude_labels)  # Set the custom text labels\n",
    "plt.tight_layout()\n",
    "# plt.savefig('zonal_mean_state_bias_v5_nopruning_noclass_drop1month_addtropopause.pdf', format='pdf', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79f681-d7f8-48ca-8a6c-7fe57593de61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0d67f-43bd-4f8f-8b1e-142cf239d48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a110c84-96b0-44d6-abb9-805ff8e83b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb8ab0-36e1-49fc-afba-9fa8e542356d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622021e-40b9-45cf-a0d5-27986c392273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3501-04ac-4523-9060-ba4c9092648b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee444e6-ba9b-47c4-ac05-ed341b78252c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98646ef1-82a3-4fce-9b3c-5c8c64f2346b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6dc04-9ad8-4df3-9e1b-3b74817535d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddbe34-8eac-454c-a553-169530cd6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm['T'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55258209-0107-4d22-a412-abe6e05e811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nn['T'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_grid = grid_info\n",
    "grid_area = ds_grid['area']\n",
    "\n",
    "def zonal_mean_area_weighted(data, grid_area, lat):\n",
    "    # Define latitude bins ranging from -90 to 90, each bin spans 10 degrees\n",
    "    bins = np.arange(-90, 91, 10)  # Create edges for 10 degree bins\n",
    "\n",
    "    # Get indices for each lat value indicating which bin it belongs to\n",
    "    bin_indices = np.digitize(lat.values, bins) - 1\n",
    "\n",
    "    # Initialize a list to store the zonal mean for each latitude bin\n",
    "    data_zonal_mean = []\n",
    "\n",
    "    # Iterate through each bin to calculate the weighted average\n",
    "    for i in range(len(bins)-1):\n",
    "        # Filter data and grid_area for current bin\n",
    "        mask = (bin_indices == i)\n",
    "        data_filtered = data[mask]\n",
    "        grid_area_filtered = grid_area[mask]\n",
    "\n",
    "        # Check if there's any data in this bin\n",
    "        if data_filtered.size > 0:\n",
    "            # Compute area-weighted average for the current bin\n",
    "            weighted_mean = np.average(data_filtered, axis=0, weights=grid_area_filtered)\n",
    "        else:\n",
    "            # If no data in bin, append NaN or suitable value\n",
    "            weighted_mean = np.nan\n",
    "\n",
    "        # Append the result to the list\n",
    "        data_zonal_mean.append(weighted_mean)\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    data_zonal_mean = np.array(data_zonal_mean)\n",
    "\n",
    "    # The mid points of the bins are used as the representative latitudes\n",
    "    lats_mid = bins[:-1] + 5\n",
    "\n",
    "    return data_zonal_mean, lats_mid\n",
    "\n",
    "ds2 = xr.open_dataset(data_path+'data_grid/E3SM_ML.GNUGPU.F2010-MMF1.ne4pg2_ne4pg2.eam.h0.0001-01.nc')\n",
    "lat = ds2.lat\n",
    "lon = ds2.lon\n",
    "level = ds2.lev.values\n",
    "\n",
    "def zonal_mean(var):\n",
    "    var_re = var.reshape(-1,384,var.shape[-1])\n",
    "    var_re = np.transpose(var_re, (1,0,2))\n",
    "    var_zonal_mean, lats_sorted = zonal_mean_area_weighted(var_re, grid_area, lat)\n",
    "    return var_zonal_mean, lats_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266f9d2-f0a4-4036-9c39-163db1df5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b17c94-cea4-4a9b-84ca-1ef43559d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmf_file = xr.open_dataset('/pscratch/sd/z/zeyuanhu/hu_etal2024_data/h0/5year/mmf_ref/control_fullysp_jan_wmlio_r3.eam.h0.0003-01.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85674e2-c3db-48e6-a511-22b64b1675ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = data_path+'h0/5year/unet_v5/huber_rop/*.eam.h0.000[3-8]*.nc'\n",
    "ds_nn = xr.open_mfdataset(filenames)\n",
    "\n",
    "# Exclude the first month (0003-01) due to spin model\n",
    "ds_nn = ds_nn.sel(time=ds_nn.time[1:])\n",
    "ds_nn['lev'].attrs['long_name'] = 'hybrid pressure'\n",
    "\n",
    "filenames = data_path+'h0/5year/mmf_ref/*.eam.h0.000[3-8]*.nc'\n",
    "ds_sp = xr.open_mfdataset(filenames)\n",
    "# Exclude the first month (0003-01) due to spin model\n",
    "ds_sp = ds_sp.sel(time=ds_sp.time[1:])\n",
    "ds_sp['lev'].attrs['long_name'] = 'hybrid pressure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdc6d8-7289-46c7-9943-c9294442967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_p400_t10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve precomputed 1-year tropopause level distribution (we used as a microphysics constraint)\n",
    "idx_p400_t10 = np.load(data_path+'microphysics_hourly/first_true_indices_p400_t10.npy')\n",
    "for i in range(idx_p400_t10.shape[0]):\n",
    "    for j in range(idx_p400_t10.shape[1]):\n",
    "        idx_p400_t10[i,j] = level[int(idx_p400_t10[i,j])]\n",
    "\n",
    "idx_p400_t10 = idx_p400_t10.mean(axis=0)\n",
    "idx_p400_t10 = idx_p400_t10[:,np.newaxis]\n",
    "\n",
    "idx_tropopause_zm = zonal_mean_area_weighted(idx_p400_t10, grid_area, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a19b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import string\n",
    "\n",
    "\n",
    "# List of variables and their settings\n",
    "variables = [\n",
    "    {'var': 'T', 'var_title': 'T', 'scaling': 1., 'unit': 'K', 'diff_scale': 0.9, 'max_diff': 5},\n",
    "    {'var': 'Q', 'var_title': 'Q', 'scaling': 1000., 'unit': 'g/kg', 'diff_scale': 1, 'max_diff': 1},\n",
    "    {'var': 'U', 'var_title': 'U', 'scaling': 1., 'unit': 'm/s', 'diff_scale': 0.2, 'max_diff': 4},\n",
    "    {'var': 'CLDLIQ', 'var_title': 'Liquid cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'diff_scale': 1, 'max_diff': 40},\n",
    "    {'var': 'CLDICE', 'var_title': 'Ice cloud', 'scaling': 1e6, 'unit': 'mg/kg', 'diff_scale': 1, 'max_diff': 5}\n",
    "]\n",
    "\n",
    "latitude_ticks = [-60, -30, 0, 30, 60]\n",
    "latitude_labels = ['60S', '30S', '0', '30N', '60N']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(5, 3, figsize=(14, 12.5)) \n",
    "# Generate the panel labels\n",
    "labels = [f\"({letter})\" for letter in string.ascii_lowercase[:15]]\n",
    "\n",
    "\n",
    "# Loop through each variable and its corresponding subplot row\n",
    "for idx, var_info in enumerate(variables):\n",
    "    var = var_info['var']\n",
    "    var_title = var_info['var_title']\n",
    "    scaling = var_info['scaling']\n",
    "    unit = var_info['unit']\n",
    "    diff_scale = var_info['diff_scale']\n",
    "\n",
    "    # Compute the means and differences for plots\n",
    "    sp_tmean = ds_sp[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    nn_tmean = ds_nn[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    sp_zm, lats_sorted = zonal_mean_area_weighted(sp_tmean, grid_area, lat)\n",
    "    nn_zm, lats_sorted = zonal_mean_area_weighted(nn_tmean, grid_area, lat)\n",
    "    \n",
    "    \n",
    "    data_sp = scaling * xr.DataArray(sp_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_nn = scaling * xr.DataArray(nn_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_diff = data_nn - data_sp\n",
    "    \n",
    "    # Determine color scales\n",
    "    vmax = max(abs(data_sp).max(), abs(data_nn).max())\n",
    "    vmin = min(abs(data_sp).min(), abs(data_nn).min())\n",
    "    # if var_info['diff_scale']:\n",
    "    #     vmax_diff = abs(data_diff).max() * diff_scale\n",
    "    #     vmin_diff = -vmax_diff\n",
    "    vmax_diff = var_info['max_diff']\n",
    "    vmin_diff = -vmax_diff\n",
    "    # Plot each variable in its row\n",
    "    data_sp.plot(ax=axs[idx, 0], add_colorbar=True, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 0].set_title(f'{labels[idx * 3]} {var_title} ({unit}): MMF')\n",
    "    axs[idx, 0].invert_yaxis()\n",
    "\n",
    "    data_nn.plot(ax=axs[idx, 1], add_colorbar=True, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 1].set_title(f'{labels[idx * 3 + 1]} {var_title} ({unit}): NN')\n",
    "    axs[idx, 1].invert_yaxis()\n",
    "    axs[idx, 1].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "\n",
    "    data_diff.plot(ax=axs[idx, 2], add_colorbar=True, cmap='RdBu_r', vmin=vmin_diff, vmax=vmax_diff)\n",
    "    axs[idx, 2].set_title(f'{labels[idx * 3 + 2]} {var_title} ({unit}): NN - MMF')\n",
    "    axs[idx, 2].invert_yaxis()\n",
    "    axs[idx, 2].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "    \n",
    "    axs[idx, 0].set_xlabel('')\n",
    "    axs[idx, 1].set_xlabel('')\n",
    "    axs[idx, 2].set_xlabel('')\n",
    "\n",
    "tropopause_pressure = idx_tropopause_zm[0].flatten()  # Flatten to 1D array\n",
    "tropopause_latitude = idx_tropopause_zm[1]  # Latitude values\n",
    "axs[4, 2].plot(tropopause_latitude, tropopause_pressure, 'k--')\n",
    "\n",
    "# Set these ticks and labels for each subplot\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xticks(latitude_ticks)  # Set the positions for the ticks\n",
    "        ax.set_xticklabels(latitude_labels)  # Set the custom text labels\n",
    "plt.tight_layout()\n",
    "# plt.savefig('zonal_mean_state_bias_v5_nopruning_noclass_drop1month_addtropopause.pdf', format='pdf', dpi=400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e83d7f-5b39-42e4-b43d-531ac0da1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import string\n",
    "\n",
    "# List of variables and their settings\n",
    "variables = [\n",
    "    {'var': 'DTPHYS', 'var_title': 'dT/dt', 'scaling': 1., 'unit': 'K/s', 'diff_scale': 1, 'state_scale': 0.5},\n",
    "    {'var': 'DQ1PHYS', 'var_title': 'dQ/dt', 'scaling': 1e3, 'unit': 'g/kg/s', 'diff_scale': 1, 'state_scale': 0.5},\n",
    "    {'var': 'DUPHYS', 'var_title': 'dU/dt', 'scaling': 1., 'unit': 'm/s²', 'diff_scale': 0.25, 'state_scale': 0.16},\n",
    "    {'var': 'DQnPHYS', 'var_title': 'dQn/dt', 'scaling': 1e6, 'unit': 'mg/kg/s', 'diff_scale': 0.5, 'state_scale': 0.6}\n",
    "]\n",
    "\n",
    "# Combine dQc/dt and dQi/dt into dQn/dt\n",
    "ds_sp['DQnPHYS'] = ds_sp['DQ2PHYS'] + ds_sp['DQ3PHYS']\n",
    "ds_nn['DQnPHYS'] = ds_nn['DQ2PHYS'] + ds_nn['DQ3PHYS']\n",
    "\n",
    "# Create a figure with subplots (4 rows x 3 columns) to rotate the figure\n",
    "fig, axs = plt.subplots(4, 3, figsize=(14, 12))  # Adjust size as necessary\n",
    "# Generate the panel labels\n",
    "labels = [f\"({letter})\" for letter in string.ascii_lowercase[:12]]\n",
    "\n",
    "# Loop through each variable and its corresponding subplot position\n",
    "for idx, var_info in enumerate(variables):\n",
    "    var = var_info['var']\n",
    "    var_title = var_info['var_title']\n",
    "    scaling = var_info['scaling']\n",
    "    unit = var_info['unit']\n",
    "    diff_scale = var_info['diff_scale']\n",
    "    state_scale = var_info['state_scale']\n",
    "\n",
    "    # Compute the means and differences for plots\n",
    "    sp_tmean = ds_sp[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    nn_tmean = ds_nn[var].mean(dim=('time')).compute().transpose('ncol', 'lev')\n",
    "    sp_zm, lats_sorted = zonal_mean_area_weighted(sp_tmean, grid_area, lat)\n",
    "    nn_zm, lats_sorted = zonal_mean_area_weighted(nn_tmean, grid_area, lat)\n",
    "    \n",
    "    data_sp = scaling * xr.DataArray(sp_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_nn = scaling * xr.DataArray(nn_zm[:, :].T, dims=[\"hybrid pressure (hPa)\", \"latitude\"],\n",
    "                                     coords={\"hybrid pressure (hPa)\": level, \"latitude\": lats_sorted})\n",
    "    data_diff = data_nn - data_sp\n",
    "\n",
    "    # Determine color scales\n",
    "    vmax = max(abs(data_sp).max(), abs(data_nn).max())\n",
    "    vmin = min(abs(data_sp).min(), abs(data_nn).min())\n",
    "    \n",
    "    vmax = max(abs(vmax), abs(vmin)) * state_scale\n",
    "    vmin = -vmax\n",
    "    \n",
    "    # Plot MMF for each variable in the first row\n",
    "    data_sp.plot(ax=axs[idx, 0], add_colorbar=True, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 0].set_title(f'{labels[idx*3]} {var_title} ({unit}): MMF')\n",
    "    axs[idx, 0].invert_yaxis()\n",
    "\n",
    "    # Plot NN for each variable in the second row\n",
    "    data_nn.plot(ax=axs[idx, 1], add_colorbar=True, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    axs[idx, 1].set_title(f'{labels[idx*3 + 1]} {var_title} ({unit}): NN')\n",
    "    axs[idx, 1].invert_yaxis()\n",
    "\n",
    "    # Plot NN-MMF for each variable in the third row\n",
    "    vmax_diff = max(abs(data_diff).max(), abs(data_diff).min()) * diff_scale\n",
    "    vmin_diff = -vmax_diff\n",
    "    data_diff.plot(ax=axs[idx, 2], add_colorbar=True, cmap='RdBu_r', vmin=vmin_diff, vmax=vmax_diff)\n",
    "    axs[idx, 2].set_title(f'{labels[idx*3 + 2]} {var_title} ({unit}): NN - MMF')\n",
    "    axs[idx, 2].invert_yaxis()\n",
    "\n",
    "    # Clear x-labels to clean up plot\n",
    "    axs[idx, 0].set_xlabel('')\n",
    "    axs[idx, 1].set_xlabel('')\n",
    "    axs[idx, 2].set_xlabel('')\n",
    "    \n",
    "    if idx > 0:\n",
    "        axs[idx, 0].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "        axs[idx, 1].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "        axs[idx, 2].set_ylabel('')  # Clear the y-label to clean up plot\n",
    "\n",
    "# Set these ticks and labels for each subplot\n",
    "latitude_ticks = [-60, -30, 0, 30, 60]\n",
    "latitude_labels = ['60S', '30S', '0', '30N', '60N']\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xticks(latitude_ticks)  # Set the positions for the ticks\n",
    "        ax.set_xticklabels(latitude_labels)  # Set the custom text labels\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('zonal_mean_tendency_bias_reduced_nopruning_noclass_wdiff_vertical_drop1month.pdf', format='pdf', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed87488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f8c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a13f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864db76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6941e5c-270c-481b-bbfb-e319f3edf05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from climsim_utils.data_utils import *\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f20011-5174-4d31-83ae-ad671ec3cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_info = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/grid_info/ClimSim_low-res_grid-info.nc')\n",
    "input_mean = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_mean_v2_rh_mc_pervar.nc')\n",
    "input_max = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_max_v2_rh_mc_pervar.nc')\n",
    "input_min = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_min_v2_rh_mc_pervar.nc')\n",
    "output_scale = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/outputs/output_scale_std_lowerthred_v2_rh_mc.nc')\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)\n",
    "data.set_to_v2_rh_mc_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de6b0f-a9d6-48d5-bd71-3f44403b002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827c1dc-d8d6-4078-bf59-64876fcfa41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_paths = [f'/pscratch/sd/j/jerrylin/hugging/E3SM-MMF_ne4/preprocessing/v2_rh_mc/train_set/{x}/' for x in \\\n",
    "                   ['11', '12', '21', '22', '31', '32', '41', '42', '51', '52', '61', '62', '71', '72']]\n",
    "train_input = np.concatenate([np.load(f'{train_set_path}/train_input.npy') for train_set_path in train_set_paths], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0ebdb-45f2-46d7-b0bc-595e03cc76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_mean = np.mean(train_input, axis = 0)\n",
    "npy_std = np.std(train_input, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb2576-be85-4ea6-a4e7-dc33e2188c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_info = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/grid_info/ClimSim_low-res_grid-info.nc')\n",
    "input_mean = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_mean_v2_rh_mc_pervar.nc')\n",
    "input_max = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_max_v2_rh_mc_pervar.nc')\n",
    "input_min = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_min_v2_rh_mc_pervar.nc')\n",
    "output_scale = xr.open_dataset('/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/outputs/output_scale_std_lowerthred_v2_rh_mc.nc')\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)\n",
    "data.set_to_v2_rh_mc_vars()\n",
    "input_sub, input_div, out_scale = data.save_norm(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc0218-dbab-42da-982c-46166d585831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49d439-44b6-4931-87bf-5f525087eb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5f11f-1e19-4632-abcd-63f2c413525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4222d8-43f2-4dc4-9c35-eea96316ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20000000\n",
    "np.sum(train_input[:num_samples,:10], axis=0)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664450b-8685-4604-96ee-548a2b075b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_input[:,0], bins=100)\n",
    "plt.show()\n",
    "#np.nanmean(train_input[:,:10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354b184-89dc-46cc-bfb5-e1dbc4ab0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[:,0][:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55508f-1458-4351-9e58-ef370a602bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_input, axis = 0)[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab792a82-effa-4d4b-ac44-87f670f822d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_input, axis = 0, dtype = np.float64)[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200d0d5-9c8f-4808-acca-def6da402675",
   "metadata": {},
   "outputs": [],
   "source": [
    "16777215/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84f90f-d594-4ad6-afce-9aa33a283781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npy_mean[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fe729-1508-4e31-8048-9a029debd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_sub[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc2ec5-74ce-4f77-8bb3-3405932057c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = {\n",
    "input_mean_path: '/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_mean_v2_rh_mc_pervar.nc'\n",
    "input_max_path: '/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_max_v2_rh_mc_pervar.nc'\n",
    "input_min_path: '/global/cfs/cdirs/m4334/jerry/climsim3_dev/preprocessing/normalizations/inputs/input_min_v2_rh_mc_pervar.nc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598393a-830e-4c71-aae3-dc3fa3c845eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "10089984/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416277d-8fcb-4297-ab50-240b16ce1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "woah.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82db83-7ae0-423b-b994-7df5d734b101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instantiating class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542b197-e371-4346-94b4-bc5ecfcf0f82",
   "metadata": {},
   "source": [
    "The example below will save training data in both .h5 and .npy format. Adjust if you only need one format. Also adjust input_abbrev to the input data files you will use. We expanded the original '.mli.' input files to include additional features such as previous steps' information, and '.mlexpand.' was just an arbitrary name we used for the expanded input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cf98d-4871-4a02-ba6a-fe90df706d5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Currently the training script would assume the training set is in .h5 format while the validation set is in .npy form. It's fine to only keep save_h5=True in the block below for generating training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4baee2-c25e-4e14-bae4-038e67a40740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_path = '/global/u2/z/zeyuanhu/nvidia_codes/Climsim_private/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = '/global/u2/z/zeyuanhu/nvidia_codes/Climsim_private/preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "#no naming issue here. Here these normalization-related files are just placeholders since we set normalize=False in the data_utils.\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean_v5_pervar.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max_v5_pervar.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min_v5_pervar.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale_std_lowerthred_v5.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cf9ea-41d1-4b72-bff1-9a900188e834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7a139-d2f7-4229-8360-9f7f0422703e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d01fa-eed6-493b-9e66-65b43796354b",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab985d2d-ce4b-4bfd-81cd-c67d9502a2fb",
   "metadata": {},
   "source": [
    "Below is an example of creating the training data by integrating the 7 year climsim simulation data. A subsampling of 1000 is used as an example. In the actual work we did, we used a stride_sample=1. We could not fit the full 7-year data into the memory wihout subsampling. If that's also the case for you, try to only process a subset of data at one time by adjusting regexps in set_regexps method. We saved 14 separate input .h5 files. For each year, we saved two files by setting start_idx=0 or 1. We have a folder like v2_full, which includes 14 subfolders named '11', '12', '21', '22', ..., '71','72', and each subfolder contains a train_input.h5 and train_target.h5. How you split to save training data won't influence the training. The training script will read in all the samples and randomly select samples across all the samples to form each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c633a-cad8-4cce-9f40-7f4acff845a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set regular expressions for selecting training data\n",
    "data.set_regexps(data_split = 'train', \n",
    "                regexps = ['E3SM-MMF.mlexpand.000[1234567]-*-*-*.nc', # years 1 through 7\n",
    "                        'E3SM-MMF.mlexpand.0008-01-*-*.nc']) # first month of year 8\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split = 'train', stride_sample = 1000)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'train', start_idx=0)\n",
    "# save numpy files of training data\n",
    "data.save_as_npy(data_split = 'train', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc28f9-f333-4433-b9cc-8d0ecc3d7f07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cafa5c-0117-45e5-9488-0e2923f498f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set regular expressions for selecting validation data\n",
    "data.set_regexps(data_split = 'val',\n",
    "                 regexps = ['E3SM-MMF.mlexpand.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
    "                            'E3SM-MMF.mlexpand.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
    "                            'E3SM-MMF.mlexpand.0009-01-*-*.nc']) # first month of year 9\n",
    "# set temporal subsampling\n",
    "# data.set_stride_sample(data_split = 'val', stride_sample = 7)\n",
    "data.set_stride_sample(data_split = 'val', stride_sample = 700)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'val')\n",
    "# save numpy files of validation data\n",
    "data.save_as_npy(data_split = 'val', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd7827-3210-444e-be21-9126518c3cc6",
   "metadata": {},
   "source": [
    "### Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c81c8b-486b-4fab-8167-24e55b4c7719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.data_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/_test/'\n",
    "\n",
    "data.set_to_v4_vars()\n",
    "\n",
    "# set regular expressions for selecting validation data\n",
    "data.set_regexps(data_split = 'test',\n",
    "                 regexps = ['E3SM-MMF.mlexpand.0009-0[3456789]-*-*.nc', \n",
    "                            'E3SM-MMF.mlexpand.0009-1[012]-*-*.nc',\n",
    "                            'E3SM-MMF.mlexpand.0010-*-*-*.nc',\n",
    "                            'E3SM-MMF.mlexpand.0011-0[12]-*-*.nc'])\n",
    "# set temporal subsampling\n",
    "# data.set_stride_sample(data_split = 'test', stride_sample = 7)\n",
    "data.set_stride_sample(data_split = 'test', stride_sample = 700)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'test')\n",
    "# save numpy files of validation data\n",
    "data.save_as_npy(data_split = 'test', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d01b8-b20c-4dec-a967-981f6ecf514b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnvironment",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
